è¿™æ˜¯ä¸€ä¸ªä¸º **Probabilistic G-BERT (System V3)** é‡èº«å®šåˆ¶çš„å·¥ç¨‹å®ç°è®¾è®¡æ–‡æ¡£ã€‚

è¿™ä»½æ–‡æ¡£å°†æŒ‡å¯¼ä½ ï¼ˆæˆ– Claude Codeï¼‰æ„å»ºæ•´ä¸ªé¡¹ç›®ã€‚å®ƒé‡‡ç”¨äº†æ¨¡å—åŒ–è®¾è®¡ï¼Œç¡®ä¿ä»£ç ç»“æ„æ¸…æ™°ã€æ˜“äºè°ƒè¯•ï¼Œå¹¶ä¸”å†…ç½®äº† **è‡ªåŠ¨ç¡¬ä»¶æ£€æµ‹ (GPU/CPU)** é€»è¾‘ã€‚

---

# Probabilistic G-BERT: å·¥ç¨‹å®ç°è®¾è®¡æ–‡æ¡£ (Implementation Design Doc)

## 1. é¡¹ç›®æ¦‚è§ˆ (Project Overview)

* **é¡¹ç›®åç§°:** `probabilistic_gbert`
* **æ ¸å¿ƒæ¶æ„:** Bottlenecked Tri-Branch (RoBERTa â†’ 64d Semantic Î¼ â†’ Mass Îº + Aux logits)
* **æ¶æ„æµå‘:** **ä¸²è¡Œç»“æ„** - Branch C æ¥åœ¨ Branch A è¾“å‡ºç«¯ï¼Œå½¢æˆä¿¡æ¯ç“¶é¢ˆçº¦æŸ
* **è®­ç»ƒç›®æ ‡:** 3-Part Loss (vMF-NCE + Calibration + Auxiliary)
* **ç¡¬ä»¶ç­–ç•¥:** ä¼˜å…ˆä½¿ç”¨ CUDA (GPU)ï¼Œè‹¥ä¸å¯ç”¨è‡ªåŠ¨å›é€€è‡³ CPUã€‚
* **å¼€å‘æ¡†æ¶:** Python 3.9+, PyTorch 2.0+, Transformers

---

## 2. é¡¹ç›®ç›®å½•ç»“æ„ (Directory Structure)

```text
probabilistic_gbert/
â”œâ”€â”€ data/                        # æ•°æ®å­˜å‚¨
â”‚   â”œâ”€â”€ raw/                     # åŸå§‹æ•°æ®
â”‚   â”œâ”€â”€ processed/               # å¤„ç†åçš„ JSONL (å¸¦ Soft Labels)
â”‚   â””â”€â”€ generate_data.py         # LLM æ•°æ®ç”Ÿæˆè„šæœ¬ (éœ€åŒ…å« PRD ä¸­çš„ Soft Label Prompt)
â”œâ”€â”€ src/                         # æºä»£ç 
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                # å…¨å±€é…ç½®å‚æ•° (Hyperparameters)
â”‚   â”œâ”€â”€ dataset.py               # æ•°æ®åŠ è½½ä¸å¤„ç† (Soft Label Max-Norm)
â”‚   â”œâ”€â”€ model.py                 # PyTorch æ¨¡å‹å®šä¹‰ (Tri-Branch ä¸²è¡Œç»“æ„)
â”‚   â”œâ”€â”€ loss.py                  # æŸå¤±å‡½æ•°å®šä¹‰ (MATS Loss)
â”‚   â””â”€â”€ utils.py                 # å·¥å…·å‡½æ•° (Logger, Metrics, Device)
â”œâ”€â”€ checkpoints/                 # æ¨¡å‹ä¿å­˜è·¯å¾„
â”œâ”€â”€ train.py                     # è®­ç»ƒä¸»å…¥å£
â”œâ”€â”€ inference.py                 # æ¨ç†ä¸æ£€ç´¢æµ‹è¯•è„šæœ¬
â”œâ”€â”€ requirements.txt             # ä¾èµ–åŒ…
â””â”€â”€ README.md                    # é¡¹ç›®è¯´æ˜
```

---

## 3. æ¨¡å—åŠŸèƒ½è¯¦ç»†æè¿° (Module Specifications)

### 3.1 `src/config.py` (é…ç½®ä¸­å¿ƒ)

ç®¡ç†æ‰€æœ‰è¶…å‚æ•°ï¼Œç¡®ä¿å®éªŒå¯å¤ç°ã€‚

* **ç±»:** `Config` (ä½¿ç”¨ `dataclass`)
* **å…³é”®å‚æ•°:**

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| `model_name` | "roberta-base" | é¢„è®­ç»ƒæ¨¡å‹ |
| `embedding_dim` | 64 | Bottleneck ç»´åº¦ |
| `num_emotions` | 28 | GoEmotions ç±»åˆ«æ•° |
| `alpha_scale` | 50.0 | ç‰©ç†ç¼©æ”¾ç³»æ•° (Îº = 1.0 + Î± Ã— mass) |
| `max_length` | 128 | æ–‡æœ¬æœ€å¤§é•¿åº¦ |
| `physical_batch_size` | 64 | å•æ¬¡å‰å‘ä¼ æ’­å¤§å° (è§†æ˜¾å­˜è°ƒæ•´) |
| `effective_batch_size` | 256 | å¯¹æ¯”å­¦ä¹ ç›®æ ‡ Batch å¤§å° |
| `grad_accum_steps` | 4 | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° (Physical Ã— Accum = Effective) |
| `lr_backbone` | 2e-5 | Backbone å­¦ä¹ ç‡ |
| `lr_heads` | 1e-4 | åˆ†æ”¯å­¦ä¹ ç‡ |
| `epochs` | 5 | è®­ç»ƒè½®æ•° |
| `lambda_cal` | 0.1 | æ ¡å‡†æŸå¤±æƒé‡ |
| `lambda_aux` | 0.05 | è¾…åŠ©æŸå¤±æƒé‡ |
| `device` | åŠ¨æ€è·å– | "cuda" or "cpu" |

### 3.2 `src/dataset.py` (æ•°æ®ç®¡é“)

è´Ÿè´£åŠ è½½ JSONL æ•°æ®å¹¶è®¡ç®—å¼ºåº¦ã€‚

* **ç±»:** `EmotionDataset(Dataset)`
* **`__init__(data_path, tokenizer, max_len)`:** åŠ è½½æ•°æ®ã€‚
* **`__getitem__(idx)`:**
  1. Tokenize æ–‡æœ¬ã€‚
  2. åŠ è½½ Soft Label åˆ†å¸ƒ (List -> FloatTensor)ã€‚
  3. **æ ¸å¿ƒé€»è¾‘:** å®æ—¶è®¡ç®—å¼ºåº¦ `intensity = torch.max(soft_label)`ã€‚
  4. è¿”å›å­—å…¸: `input_ids`, `attention_mask`, `soft_label`, `intensity`ã€‚

* **å‡½æ•°:** `create_dataloaders(config)`
  * è¿”å› Train/Val DataLoaderã€‚

### 3.3 `src/model.py` (æ¨¡å‹æ¶æ„)

å®ç° System V3 çš„ä¸‰åˆ†æ”¯**ä¸²è¡Œ**ç»“æ„ã€‚

**æ¶æ„æµå‘:**
```
RoBERTa (768d)
    â”œâ”€â†’ Branch A (Semantic) â†’ Î¼ (64d) â”€â†’ Branch C (Auxiliary) â†’ logits (28d)
    â””â”€â†’ Branch B (Mass)     â†’ Îº (1d)
```

* **ç±»:** `ProbabilisticGBERT(nn.Module)`

* **`__init__(config)`:**
  * åˆå§‹åŒ– Backbone (RoBERTa)ã€‚
  * **Branch A:** `Sequential(Linear(768â†’256), GELU, Linear(256â†’64))`ã€‚
  * **Branch B:** `EnergyProj(768â†’1)`, `AttnProj(768â†’1)`ã€‚
  * **Branch C:** `Sequential(Linear(64â†’128), GELU, Linear(128â†’28))`ã€‚

* **`forward(input_ids, mask)`:**
  * æå– `[CLS]` token (768d) å’Œ `last_hidden_state` (BÃ—LÃ—768)ã€‚
  * **Branch A:** è®¡ç®— `mu` (64d)ï¼Œ**å¿…é¡»åš** `F.normalize(mu, p=2, dim=1)`ã€‚
  * **Branch B:**
    * è®¡ç®—æ³¨æ„åŠ›æƒé‡å’Œèƒ½é‡
    * åº”ç”¨å…¬å¼: `kappa = 1.0 + alpha_scale * mass`
  * **Branch C:** **ä»¥ Branch A çš„è¾“å‡º `mu` ä¸ºè¾“å…¥**ï¼Œè®¡ç®— `aux_logits` (28d)ã€‚
  * **Return:** åŒ…å« `mu`, `kappa`, `aux_logits`, `mass` çš„å­—å…¸ã€‚

**å…³é”®è®¾è®¡æ€æƒ³:** Branch C æ¥åœ¨ Branch A ä¹‹åï¼Œå¯¹ 64d ç“¶é¢ˆå‘é‡è¿›è¡Œè§£ç ã€‚å¦‚æœ Branch A ä¸¢å¤±äº†è¯­ä¹‰ä¿¡æ¯ï¼ŒBranch C çš„ KL æ•£åº¦æŸå¤±ä¼šçˆ†ç‚¸ï¼Œä»è€Œæƒ©ç½š Branch Aã€‚è¿™å½¢æˆäº†**ä¿¡æ¯ç“¶é¢ˆçº¦æŸ**ã€‚

### 3.4 `src/loss.py` (æŸå¤±å‡½æ•°)

å®ç° NIPS æ ¸å¿ƒæ•°å­¦å…¬å¼ã€‚

* **ç±»:** `GBERTLoss(nn.Module)`
* **`__init__(config)`:** ä¿å­˜æƒé‡ `lambda_cal`, `lambda_aux`ã€‚

* **`forward(outputs, soft_labels)`:**

  1. **Extract:** ä» `outputs` å­—å…¸è·å– `mu`, `kappa`, `aux_logits`ã€‚

  2. **Calc $L_{vMF}$ (ä¸»æŸå¤±):**
     * è®¡ç®— Cosine Similarity Matrix: `logits = torch.matmul(mu, mu.T)`
     * è®¡ç®—åŠ¨æ€æ¸©åº¦: `tau = 1.0 / (kappa + 1e-6)`
     * ç¼©æ”¾ Logits: `scaled_logits = logits / tau`
     * è®¡ç®— CrossEntropy (Label ä¸º `torch.arange(B)`, å‡è®¾æ¯ä¸ªæ ·æœ¬çš„ä¸‹ä¸ªæ ·æœ¬æ˜¯å…¶æ­£å¯¹)

  3. **Calc $L_{Cal}$ (æ ¡å‡†æŸå¤±):**
     * è®¡ç®— Target: `kappa_tgt = 1.0 + 50.0 * torch.max(soft_labels, dim=1).values`
     * è®¡ç®— MSE Loss: `F.mse_loss(kappa.squeeze(), kappa_tgt)`

  4. **Calc $L_{Aux}$ (è¾…åŠ©æŸå¤±):**
     * è®¡ç®— KL Divergence: `F.kl_div(F.log_softmax(aux_logits, dim=1), soft_labels, reduction='batchmean')`

  5. **Sum:** `L_total = L_vMF + lambda_cal * L_Cal + lambda_aux * L_Aux`

**çƒ­åŠ›å­¦è§£é‡Š:** åŠ¨æ€æ¸©åº¦ Ï„ ä½¿ç³»ç»Ÿåœ¨ä¸åŒ"çƒ­åŠ›å­¦æ€"é—´è‡ªé€‚åº”åˆ‡æ¢ï¼š
* **Solid State (Îº â†’ 50, Ï„ â†’ 0):** ä½æ¸©ä½ç†µï¼Œé«˜ç²¾åº¦æ£€ç´¢
* **Gaseous State (Îº â†’ 1~10, Ï„ å‡é«˜):** é«˜æ¸©é«˜ç†µï¼Œé«˜æ³›åŒ–æ€§

### 3.5 `train.py` (è®­ç»ƒå¾ªç¯)

ä¸»æ§åˆ¶æµï¼ŒåŒ…å«ç¡¬ä»¶é€‚é…é€»è¾‘ã€‚

* **åŠŸèƒ½æµç¨‹:**

  1. **ç¡¬ä»¶æ£€æµ‹:**
  ```python
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  if torch.cuda.is_available():
      torch.backends.cudnn.benchmark = True
      print(f"ğŸš€ GPU Activated: {torch.cuda.get_device_name(0)}")
  else:
      print("âš ï¸ GPU Not Found. Falling back to CPU. Training will be slow.")
  ```

  2. åˆå§‹åŒ– `Config`, `Tokenizer`, `Model`, `Loss`, `Optimizer`ã€‚
  3. æ¨¡å‹ `.to(device)`ã€‚
  4. åˆå§‹åŒ– `Optimizer` (åˆ†ç»„å­¦ä¹ ç‡ï¼šBackbone ç”¨å° LRï¼ŒHeads ç”¨å¤§ LR)ã€‚
  5. **Loop Epochs:**
     * **Loop Batches:**
       * æ•°æ® `.to(device)`ã€‚
       * Forward Passã€‚
       * Compute Lossã€‚
       * **Gradient Accumulation:**
         ```python
         loss = loss / config.grad_accum_steps
         loss.backward()
         if (step + 1) % config.grad_accum_steps == 0:
             optimizer.step()
             optimizer.zero_grad()
         ```
       * Logging (Console / WandB)ã€‚
  6. **Save Model:** ä¿å­˜ `state_dict`ã€‚

---

## 4. å…³é”®å®ç°ç»†èŠ‚ (Key Implementation Details)

### 4.1 ç¡¬ä»¶å›é€€ç­–ç•¥ (Hardware Fallback)

åœ¨æ‰€æœ‰æ¶‰åŠ Tensor è¿ç®—çš„åœ°æ–¹ï¼Œéƒ½ä¸ç¡¬ç¼–ç  `.cuda()`ï¼Œè€Œæ˜¯ä½¿ç”¨ `config.device` æˆ– `tensor.to(device)`ã€‚

```python
# ç¤ºä¾‹ï¼štrain.py
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True
    device_name = torch.cuda.get_device_name(0)
    print(f"ğŸš€ GPU Activated: {device_name}")
else:
    print("âš ï¸ GPU Not Found. Falling back to CPU. Training will be slow.")
```

### 4.2 æ•°å€¼ç¨³å®šæ€§ (Numerical Stability)

* **Kappa ä¸‹ç•Œ:** åœ¨è®¡ç®— `tau = 1/kappa` æ—¶ï¼Œé˜²æ­¢ `kappa` è¿‡å°å¯¼è‡´é™¤é›¶ã€‚
  * ä»£ç : `tau = 1.0 / (kappa + 1e-6)`

* **Softplus:** è®¡ç®—èƒ½é‡æ—¶ä½¿ç”¨ `F.softplus()` ä¿è¯è´¨é‡éè´Ÿã€‚
  * ä»£ç : `token_energies = F.softplus(self.energy_proj(last_hidden))`

### 4.3 æ¨¡æ‹Ÿå¤§ Batch (Contrastive Requirement)

å¯¹æ¯”å­¦ä¹ ä¾èµ– Batch å†…è´Ÿæ ·æœ¬æ•°é‡ã€‚

* å¦‚æœä½¿ç”¨ GPU (å¦‚ 16GB VRAM)ï¼ŒPhysical Batch Size å¯èƒ½åªèƒ½å¼€åˆ° 64ã€‚
* **å¿…é¡»å®ç°** æ¢¯åº¦ç´¯ç§¯ï¼Œé€»è¾‘å¦‚ä¸‹ï¼š
```python
loss = loss / config.grad_accum_steps  # ç¼©æ”¾ loss
loss.backward()
if (step + 1) % config.grad_accum_steps == 0:
    optimizer.step()
    optimizer.zero_grad()
```
* è¿™ä½¿å¾— **Effective Batch Size = Physical Ã— Accum = 64 Ã— 4 = 256**

---

## 5. å¼€å‘æ­¥éª¤ (Action Plan)

1. **ç¯å¢ƒå‡†å¤‡:** åˆ›å»º `requirements.txt` (torch, transformers, scikit-learn, tqdm)ã€‚
2. **æ•°æ®ç”Ÿæˆ:** ç¼–å†™ `data/generate_data.py`ï¼Œ**å¿…é¡»ä½¿ç”¨ PRD æ–‡æ¡£ä¸­æŒ‡å®šçš„ Soft Label Prompt**ï¼Œè°ƒç”¨ API å‡†å¤‡å¥½ `train.jsonl`ã€‚
3. **æ ¸å¿ƒæ¨¡å—:** ä¾æ¬¡å®ç° `config.py` â†’ `model.py` â†’ `loss.py`ã€‚
4. **è®­ç»ƒè„šæœ¬:** ç¼–å†™ `train.py` å¹¶å…ˆç”¨ CPU åœ¨å°‘é‡æ•°æ®ä¸Šè·‘é€šæµç¨‹ (Debug mode)ã€‚
5. **å…¨é‡è®­ç»ƒ:** åˆ‡æ¢åˆ° GPU è¿›è¡Œå®Œæ•´è®­ç»ƒã€‚
