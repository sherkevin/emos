# DESGINï¼šProbabilistic G-BERT: å·¥ç¨‹å®ç°è®¾è®¡æ–‡æ¡£ (Implementation Design Doc)

## 1. é¡¹ç›®æ¦‚è§ˆ (Project Overview)

* **é¡¹ç›®åç§°:** `probabilistic_gbert`
* **æ ¸å¿ƒæ¶æ„:** Bottlenecked Tri-Branch (RoBERTa â†’ Token-Level Projection â†’ Masked Pooling â†’ 64d Î¼ / 1d Îº)
* **æ¶æ„æµå‘:** **ä¸²è¡Œç»“æ„** - Branch C æ¥åœ¨ Branch A çš„ Pooled è¾“å‡ºç«¯ï¼Œå½¢æˆä¿¡æ¯ç“¶é¢ˆçº¦æŸ
* **è®­ç»ƒç›®æ ‡:** 3-Part Loss (vMF-NCE + Calibration + Auxiliary)
* **å¯¹æ¯”å­¦ä¹ ç­–ç•¥:** **Supervised vMF-NCE** - Class-Prototype ç›‘ç£ä¿¡å·ï¼Œæ‹‰è¿‘æ ·æœ¬ä¸æƒ…æ„Ÿç±»åˆ«ä¸­å¿ƒçš„è·ç¦»
* **å®ä½“æ”¯æŒ (V4 æ–°å¢):** Token ç²’åº¦ Entity Masking + Sample Flattening + Character Offsetsï¼Œæ”¯æŒå®ä½“çº§è¯­ä¹‰æå–
* **ç¡¬ä»¶ç­–ç•¥:** ä¼˜å…ˆä½¿ç”¨ CUDA (GPU)ï¼Œè‹¥ä¸å¯ç”¨è‡ªåŠ¨å›é€€è‡³ CPUã€‚
* **å¼€å‘æ¡†æ¶:** Python 3.9+, PyTorch 2.0+, Transformers

---

## 2. å…³é”®å†³ç­–çŸ©é˜µ (Decision Matrix)

| # | é—®é¢˜ | æœ€ç»ˆå†³ç­– (Final Decision) |
|---|------|---------------------------|
| 1 | å¯¹æ¯”å­¦ä¹ ç­–ç•¥ | **Supervised vMF-NCE** - Class-Prototype ç›‘ç£ä¿¡å·ï¼Œæ‹‰è¿‘æ ·æœ¬ä¸æƒ…æ„Ÿç±»åˆ«ä¸­å¿ƒçš„è·ç¦» |
| 2 | æ•°æ®ç”Ÿæˆ | **ä¼˜å…ˆå¤ç”¨ GoEmotions Raw Votes**ï¼›LLM è„šæœ¬ç”¨ gpt-4o-mini å¤‡ç”¨ |
| 3 | LR Scheduler | **Linear Warmup (10%) + Decay** |
| 4 | Checkpoint | ä¿å­˜ **best_model.pt** (æƒé‡) å’Œ **last.pt** (å…¨çŠ¶æ€) |
| 5 | Logging | **Console (Standard) + WandB (If available)** |
| 6 | Masking (V4) | **Token-Level Projection** + **Masked Pooling**ï¼›æ”¯æŒ `entity_mask` å®ä½“æå– |
| 7 | æ¨ç†åŠŸèƒ½ | **å•å¥åˆ†ææ¨¡å¼** (entity_mask=None) + **å®ä½“æå–æ¨¡å¼** (entity_mask æŒ‡å®š) |
| 8 | Batch Size | æ˜¾å­˜å…è®¸ä¸‹çš„æœ€å¤§å€¼ (å¦‚ 64) + æ¢¯åº¦ç´¯ç§¯ (è‡³ 256) |
| 9 | Pooling ç­–ç•¥ (V4) | Branch A: **Mean Pool**ï¼›Branch B: **Max Pool** (ä¸“æ³¨åº¦æ˜¯å¼ºåº¦é‡) |
| 10 | Token å¯¹é½ (V4) | **Character Offsets** - ä½¿ç”¨ `char_start`, `char_end` ç²¾ç¡®å®šä½ï¼Œé¿å…é‡å¤è¯æ­§ä¹‰ |
| 11 | æ•°æ®é‡‡æ · (V4) | **Sample Flattening** - `__init__` æ—¶å±•å¹³æ‰€æœ‰ targetsï¼Œ1 å¥ N å®ä½“ â†’ N ä¸ªè®­ç»ƒæ ·æœ¬ |
| 12 | è®­ç»ƒç›‘ç£ (V4) | **Supervised Multi-Granularity** - ç›´æ¥åœ¨å®ä½“çº§åˆ«è®¡ç®— Lossï¼Œé Zero-Shot |

---

## 3. é¡¹ç›®ç›®å½•ç»“æ„ (Directory Structure)

```text
probabilistic_gbert/
â”œâ”€â”€ data/                        # æ•°æ®å­˜å‚¨
â”‚   â”œâ”€â”€ raw/                     # åŸå§‹æ•°æ® (GoEmotions)
â”‚   â”œâ”€â”€ processed/               # å¤„ç†åçš„ JSONL (å¸¦ Soft Labels)
â”‚   â””â”€â”€ generate_data.py         # LLM æ•°æ®ç”Ÿæˆè„šæœ¬ (gpt-4o-mini, å¤‡ç”¨)
â”œâ”€â”€ src/                         # æºä»£ç 
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                # å…¨å±€é…ç½®å‚æ•° (Hyperparameters)
â”‚   â”œâ”€â”€ dataset.py               # æ•°æ®åŠ è½½ä¸å¤„ç† (GoEmotions raw votes)
â”‚   â”œâ”€â”€ model.py                 # PyTorch æ¨¡å‹å®šä¹‰ (Tri-Branch ä¸²è¡Œç»“æ„)
â”‚   â”œâ”€â”€ loss.py                  # æŸå¤±å‡½æ•°å®šä¹‰ (Supervised vMF-NCE + Calibration + Auxiliary)
â”‚   â””â”€â”€ utils.py                 # å·¥å…·å‡½æ•° (Logger, Metrics, Device)
â”œâ”€â”€ checkpoints/                 # æ¨¡å‹ä¿å­˜è·¯å¾„
â”‚   â”œâ”€â”€ best_model.pt            # æœ€ä¼˜æ¨¡å‹æƒé‡ (ç”¨äºæ¨ç†)
â”‚   â””â”€â”€ last_checkpoint.pt       # æœ€æ–°å®Œæ•´æ£€æŸ¥ç‚¹ (ç”¨äºæ–­ç‚¹ç»­è®­)
â”œâ”€â”€ train.py                     # è®­ç»ƒä¸»å…¥å£
â”œâ”€â”€ inference.py                 # äº¤äº’å¼æ¨ç† Demo
â”œâ”€â”€ requirements.txt             # ä¾èµ–åŒ…
â””â”€â”€ README.md                    # é¡¹ç›®è¯´æ˜
```

---

## 4. æ¨¡å—åŠŸèƒ½è¯¦ç»†æè¿° (Module Specifications)

### 4.1 `src/config.py` (é…ç½®ä¸­å¿ƒ)

ç®¡ç†æ‰€æœ‰è¶…å‚æ•°ï¼Œç¡®ä¿å®éªŒå¯å¤ç°ã€‚

* **ç±»:** `Config` (ä½¿ç”¨ `dataclass`)

* **å…³é”®å‚æ•°:**

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| `model_name` | "roberta-base" | é¢„è®­ç»ƒæ¨¡å‹ |
| `embedding_dim` | 64 | Bottleneck ç»´åº¦ |
| `num_emotions` | 28 | GoEmotions ç±»åˆ«æ•° |
| `alpha_scale` | 50.0 | ç‰©ç†ç¼©æ”¾ç³»æ•° (Îº = 1.0 + Î± Ã— mass) |
| `max_length` | 128 | æ–‡æœ¬æœ€å¤§é•¿åº¦ |
| `physical_batch_size` | 64 | å•æ¬¡å‰å‘ä¼ æ’­å¤§å° (è§†æ˜¾å­˜è°ƒæ•´) |
| `effective_batch_size` | 256 | å¯¹æ¯”å­¦ä¹ ç›®æ ‡ Batch å¤§å° (Physical Ã— Accum) |
| `grad_accum_steps` | 4 | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° (64 Ã— 4 = 256) |
| `lr_backbone` | 2e-5 | Backbone å­¦ä¹ ç‡ |
| `lr_heads` | 1e-4 | åˆ†æ”¯å­¦ä¹ ç‡ |
| `epochs` | 5 | è®­ç»ƒè½®æ•° |
| `warmup_ratio` | 0.1 | Warmup å æ€»æ­¥æ•°çš„æ¯”ä¾‹ |
| `lambda_cal` | 0.1 | æ ¡å‡†æŸå¤±æƒé‡ |
| `lambda_aux` | 0.05 | è¾…åŠ©æŸå¤±æƒé‡ |
| `patience` | 3 | Early Stopping å®¹å¿è½®æ•° |
| `device` | åŠ¨æ€è·å– | "cuda" or "cpu" |

### 4.2 `src/dataset.py` (æ•°æ®ç®¡é“)

**æ•°æ®æ¥æºä¼˜å…ˆçº§:** GoEmotions Raw Annotations â†’ LLM ç”Ÿæˆ

#### V4 æ•°æ®æ ¼å¼ä¸å­—ç¬¦å¯¹é½ (Critical)

**å…³é”®å˜æ›´ï¼šè§£å†³ Token å¯¹é½é—®é¢˜**

å½“å¥ä¸­å­˜åœ¨é‡å¤è¯æ—¶ï¼Œä»…å‡­ `span_text` æ— æ³•ç¡®å®šæ ‡ç­¾å¯¹åº”å“ªä¸ª Tokenã€‚å¿…é¡»ä½¿ç”¨ **Character Offsets** ç²¾ç¡®å®šä½å®ä½“è¾¹ç•Œã€‚

**JSONL æ ¼å¼ (å¤šç›®æ ‡ Span with Character Offsets):**
```json
{
  "text": "The cat ate the cat food.",
  "targets": [
    {
      "span_text": "cat",
      "char_start": 4,
      "char_end": 7,
      "soft_label": {"joy": 0.8, "neutral": 0.2}
    },
    {
      "span_text": "cat food",
      "char_start": 16,
      "char_end": 24,
      "soft_label": {"neutral": 0.9}
    }
  ]
}
```

**å­—æ®µè¯´æ˜ï¼š**

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `text` | string | åŸå§‹æ–‡æœ¬ |
| `targets` | list | å®ä½“çº§åˆ«æƒ…æ„Ÿæ ‡æ³¨åˆ—è¡¨ï¼ˆä¸€ä¸ªå¥å­å¯æœ‰å¤šä¸ªç›®æ ‡ï¼‰ |
| `span_text` | string | å®ä½“åŸæ–‡ï¼ˆä»…ä¾›å‚è€ƒï¼Œä¸ç”¨äºå¯¹é½ï¼‰ |
| `char_start` | int | å®ä½“åœ¨åŸæ–‡ä¸­çš„**å­—ç¬¦èµ·å§‹ä½ç½®**ï¼ˆå«ï¼‰ |
| `char_end` | int | å®ä½“åœ¨åŸæ–‡ä¸­çš„**å­—ç¬¦ç»“æŸä½ç½®**ï¼ˆä¸å«ï¼‰ |
| `soft_label` | dict | 28 ç±»åˆ«çš„ Soft Labelï¼ˆç¨€ç–è¡¨ç¤ºï¼Œä»…æ ‡æ³¨éé›¶å€¼ï¼‰ |

#### Sample Flattening ç­–ç•¥

**é—®é¢˜ï¼š** åŸå§‹æ•°æ®æ˜¯ä¸€å¯¹å¤šç»“æ„ï¼ˆ1 ä¸ªå¥å­ â†’ N ä¸ªå®ä½“ç›®æ ‡ï¼‰ï¼Œæ— æ³•ç›´æ¥å½¢æˆ GPU å¹¶è¡Œ Batchã€‚

**è§£å†³æ–¹æ¡ˆï¼š** **Sample Flattening** â€” åœ¨ Dataset é¢„å¤„ç†é˜¶æ®µå°† 1 ä¸ªå¥å­æ‹†è§£ä¸º N ä¸ªè®­ç»ƒæ ·æœ¬ã€‚

```
åŸå§‹æ•°æ®ï¼š
"The cat played but the car broke." â†’ 2 targets (cat=joy, car=anger)

å±•å¹³åï¼š
Sample 1: text="...", entity_mask=[cat], soft_label=[joy]
Sample 2: text="...", entity_mask=[car], soft_label=[anger]
```

#### GoEmotions å¤„ç†é€»è¾‘
GoEmotions åŸå§‹æ•°æ®åŒ…å«å¤šåæ ‡æ³¨è€…çš„æŠ•ç¥¨ï¼ˆå¦‚ 3/10 äººæ ‡è®°ä¸º Angerï¼‰ï¼Œè¿™æœ¬èº«å°±æ˜¯ Soft Labelï¼š
```python
# å‡è®¾åŸå§‹æ ¼å¼: {text: "...", labels: [0, 0, 3, 1, ...]} (æ¯ä¸ªç±»åˆ«çš„æ ‡æ³¨äººæ•°)
# å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ
soft_label = labels / labels.sum()
```

* **ç±»:** `FineGrainedEmotionDataset(Dataset)`

* **`__init__(data_path, tokenizer, max_len)`:`
  * ä¿å­˜ tokenizer å’Œ max_len
  * **Sample Flattening:** éå†æ‰€æœ‰ entry çš„ targetsï¼Œå°† 1 å¥ N å®ä½“ å±•å¹³ä¸º N ä¸ªç‹¬ç«‹æ ·æœ¬

* **`_create_entity_mask(text, span_text, encoding)`:` - **å­—ç¬¦å¯¹é½æ ¸å¿ƒå‡½æ•°**
  ```python
  def _create_entity_mask(self, text, span_text, encoding):
      """
      é€šè¿‡å­—ç¬¦çº§å¯¹é½ç”Ÿæˆ entity_mask

      Args:
          text: åŸå§‹æ–‡æœ¬
          span_text: å®ä½“æ–‡æœ¬ (å¦‚ "cat")
          encoding: tokenizer çš„è¾“å‡ºï¼ŒåŒ…å« offset_mapping

      Returns:
          entity_mask: (L,) Tensor, 1 for entity tokens, 0 otherwise
      """
      # ä½¿ç”¨æ˜¾å¼çš„ character offsetsï¼ˆä¸å†éœ€è¦ text.find()ï¼‰
      c_start, c_end = item['char_start'], item['char_end']
      token_starts = offsets[:, 0]
      token_ends = offsets[:, 1]

      # Token ä¸ Entity æœ‰äº¤é›† â†’ True
      entity_mask = (token_starts < c_end) & (token_ends > c_start) & attention_mask.bool()
      return entity_mask.float()
  ```

* **`__getitem__(idx)`:**
  1. åŠ è½½æ•°æ®é¡¹ `item = self.data[idx]`
  2. **Tokenize:** å¿…é¡»å¼€å¯ `return_offsets_mapping=True`
     ```python
     encoding = self.tokenizer(
         item['text'],
         max_length=self.max_len,
         padding='max_length',
         truncation=True,
         return_offsets_mapping=True  # Critical for char-to-token alignment
     )
     ```
  3. **Span é‡‡æ ·:**
     ```python
     # Sample å·²åœ¨ __init__ ä¸­å±•å¹³ï¼Œç›´æ¥ä½¿ç”¨ item['char_start'], item['char_end']
     c_start, c_end = item['char_start'], item['char_end']
     ```
  4. **ç”Ÿæˆ entity_mask:**
     ```python
     # Construct Entity Mask via Character Offsets
     offsets = encoding['offset_mapping'].squeeze(0)  # (L, 2)
     token_starts = offsets[:, 0]
     token_ends = offsets[:, 1]

     # Token ä¸ Entity æœ‰äº¤é›† â†’ True
     entity_mask = (token_starts < c_end) & (token_ends > c_start) & attention_mask.bool()
     entity_mask = entity_mask.float()
     ```
  5. **å¤„ç† Soft Label (Dict â†’ 28d Vector):**
     ```python
     label_dict = item['soft_label']
     label_vector = torch.zeros(28)

     # Map emotion name to index (need EMOTION_INDEX mapping)
     for emotion, value in label_dict.items():
         if emotion in EMOTION_INDEX:
             label_vector[EMOTION_INDEX[emotion]] = value
     ```
  6. è¿”å›å­—å…¸:
     ```python
     {
         'input_ids': torch.tensor(encoding['input_ids']),
         'attention_mask': torch.tensor(encoding['attention_mask']),
         'entity_mask': entity_mask,  # (L,) â€” å½“å‰å®ä½“çš„ mask
         'soft_label': label_vector   # (28,) â€” å½“å‰å®ä½“çš„ soft label
     }
     ```

* **Token å¯¹é½å…¬å¼:**
  ```python
  entity_mask = (token_start < char_end) & (token_end > char_start) & attention_mask
  ```

* **å‡½æ•°:** `create_dataloaders(config)`
  * è¿”å› Train/Val DataLoaderã€‚
  * **æ³¨æ„:** ä½¿ç”¨ Random Shuffle å³å¯ï¼ŒSupervised vMF-NCE ä¸éœ€è¦ç‰¹æ®Šåˆ†ç»„ã€‚

#### LLM ç”Ÿæˆè„šæœ¬ (`data/generate_data.py`)

* **ç”¨é€”:** ä¸ºé GoEmotions æ•°æ®æºç”Ÿæˆå¸¦ Span çš„ Soft Labels
* **æ¨¡å‹:** gpt-4o-mini (æˆæœ¬ä½ï¼Œè´¨é‡è¶³å¤Ÿ)
* **API Key:** `os.getenv("OPENAI_API_KEY")`

**å…³é”®ä¿®æ­£ï¼ˆLLM å­—ç¬¦è®¡æ•°å¹»è§‰ï¼‰:**
- LLM åŸºäº Token å¤„ç†ï¼Œæ— æ³•ç²¾ç¡®è®¡æ•°å­—ç¬¦
- **åŸåˆ™ï¼š** LLM ç”Ÿæˆå†…å®¹ï¼ŒPython è®¡ç®—åæ ‡
- LLM åªéœ€è¾“å‡º `span_text` å’Œ `soft_label`
- è„šæœ¬ç”¨ `text.find()` æˆ– `re.search()` åå¤„ç†è®¡ç®— `char_start`, `char_end`

* **Prompt (V5 ä¿®æ­£ç‰ˆ):** è¦æ±‚ LLM è¾“å‡º `span_text` å’Œ `soft_label`ï¼ˆä¸è¾“å‡ºå­—ç¬¦åæ ‡ï¼‰

```python
PROMPT_V5 = """
You are an emotion analysis expert. Analyze the emotional content of entities in the following text.

Text: "{text}"

Task: Identify up to 3 key entities/phrases and distribute 1.0 probability mass across the 28 emotion categories for each.

EMOTIONS = [
    # Positive
    "admiration", "amusement", "approval", "caring", "desire",
    "excitement", "gratitude", "joy", "love", "optimism",
    "pride", "relief",
    # Negative
    "anger", "annoyance", "disappointment", "disapproval", "disgust",
    "embarrassment", "fear", "grief", "nervousness", "remorse",
    "sadness",
    # Ambiguous / Cognitive
    "confusion", "curiosity", "realization", "surprise",
    # Neutral
    "neutral"
]

Output Format (JSON only):
{{
  "targets": [
    {{
      "span_text": "exact phrase from text",
      "soft_label": {{"admiration": 0.05, ...}}
    }},
    ...
  ]
}}

IMPORTANT:
- span_text must exactly match the text (use word boundaries)
- Do NOT output character indices - they will be computed automatically
"""

# åå¤„ç† Pipeline (Python)
def postprocess_llm_output(text, llm_output):
    """
    LLM è¾“å‡ºåï¼Œä½¿ç”¨ Python è®¡ç®—å­—ç¬¦åæ ‡
    """
    for target in llm_output["targets"]:
        span_text = target["span_text"]
        # ä½¿ç”¨ Python ç²¾ç¡®å®šä½
        idx = text.find(span_text)
        if idx == -1:
            # å°è¯•æ¨¡ç³ŠåŒ¹é…
            import re
            match = re.search(re.escape(span_text), text)
            if match:
                idx = match.start()
            else:
                raise ValueError(f"Span '{span_text}' not found in text")
        target["char_start"] = idx
        target["char_end"] = idx + len(span_text)
    return llm_output
```

### 4.3 `src/model.py` (æ¨¡å‹æ¶æ„) - **V4: Entity-Aware**

å®ç° System V4 çš„ä¸‰åˆ†æ”¯**ä¸²è¡Œ**ç»“æ„ï¼Œæ”¯æŒ Token ç²’åº¦çš„å®ä½“æ„ŸçŸ¥æ“ä½œã€‚

**æ¶æ„æµå‘:**
```
RoBERTa (768d) â†’ last_hidden_state (B, L, 768)
    â”œâ”€â†’ Branch A (Semantic)   â†’ semantic_seq (B, L, 64) â†’ Masked Mean Pool â†’ Î¼ (64d)
    â”œâ”€â†’ Branch B (Mass)       â†’ energy_seq (B, L, 1)   â†’ Masked Sum Pool  â†’ Îº (1d)
    â””â”€â†’ Branch C (Auxiliary)  â†’ Î¼ (64d) â†’ logits (28d)
```

* **ç±»:** `ProbabilisticGBERT(nn.Module)`

* **`__init__(config)`:**
  * åˆå§‹åŒ– Backbone (RoBERTa-base)ã€‚
  * **Branch A:** `Sequential(Linear(768â†’256), GELU, Linear(256â†’64))`ã€‚
  * **Branch B:** `Sequential(Linear(768â†’128), GELU, Linear(128â†’1))` + `Softplus`ã€‚
  * **Branch C:** `Sequential(Linear(64â†’128), GELU, Linear(128â†’28))`ã€‚
  * `alpha_scale`: ç‰©ç†ç¼©æ”¾ç³»æ•°ï¼ˆé»˜è®¤ 50.0ï¼‰ã€‚

* **`forward(input_ids, attention_mask, entity_mask=None)`:**
  ```python
  """
  Args:
      input_ids: (B, L) Token IDs
      attention_mask: (B, L) 1=Valid, 0=Padding
      entity_mask: (B, L) - 1 for tokens in the entity, 0 otherwise.
                   If None, defaults to attention_mask (whole sentence).

  Returns:
      dict with mu (B, 64), kappa (B, 1), aux_logits (B, 28), mass (B, 1)
  """
  ```

  **å†…éƒ¨å¤„ç†æµç¨‹:**

  1. **Backbone Encoding:**
     ```python
     outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)
     last_hidden = outputs.last_hidden_state  # (B, L, 768)
     ```

  2. **Dense Projection (åº”ç”¨äºæ•´ä¸ªåºåˆ—):**
     ```python
     # Branch A: Semantic projection to token-level
     semantic_seq = self.semantic_head(last_hidden)  # (B, L, 64)

     # Branch B: Energy projection to token-level
     energy_seq = F.softplus(self.energy_head(last_hidden))  # (B, L, 1)
     ```

  3. **Masking Strategy:**
     ```python
     # Determine pooling mask
     if entity_mask is None:
         pool_mask = attention_mask  # (B, L) - whole sentence
     else:
         # Entity mode: must still respect padding
         pool_mask = entity_mask * attention_mask  # (B, L)

     pool_mask = pool_mask.unsqueeze(-1)  # (B, L, 1) for broadcasting
     ```

  4. **Manual Pooling (å…³é”®æ­¥éª¤):**

     **Branch A - Mean Pooling â†’ Î¼:**
     ```python
     # Masked mean pooling
     masked_semantics = semantic_seq * pool_mask  # (B, L, 64)
     sum_semantics = masked_semantics.sum(dim=1)  # (B, 64)
     valid_counts = pool_mask.sum(dim=1).clamp(min=1e-9)  # (B, 1)
     mu_raw = sum_semantics / valid_counts  # (B, 64)

     # L2 Normalize AFTER pooling
     mu = F.normalize(mu_raw, p=2, dim=1)  # (B, 64), ||Î¼|| = 1
     ```

     **Branch B - Max Pooling â†’ Îº (ä¸“æ³¨åº¦æ˜¯å¼ºåº¦é‡):**
     ```python
     # Masked max pooling - ä¸“æ³¨åº¦æ˜¯å¼ºåº¦é‡ï¼Œä¸å¥å­é•¿åº¦æ— å…³
     masked_energies = energy_seq * pool_mask  # (B, L, 1)
     # å°† padding ä½ç½®çš„ energy è®¾ä¸º -infï¼Œç¡®ä¿ max ä¸é€‰åˆ°å®ƒä»¬
     masked_energies = masked_energies.masked_fill(pool_mask == 0, float('-inf'))
     mass = masked_energies.max(dim=1).values  # (B, 1)

     # Physical scaling
     kappa = 1.0 + self.alpha_scale * mass  # (B, 1)
     ```

  5. **Branch C (Auxiliary):**
     ```python
     # Connect to pooled mu for supervision
     aux_logits = self.aux_head(mu)  # (B, 28)
     ```

  6. **Return:**
     ```python
     return {
         "mu": mu,              # (B, 64) - semantic direction (entity or sentence)
         "kappa": kappa,        # (B, 1)  - concentration parameter
         "mass": mass,          # (B, 1)  - raw mass (for visualization)
         "aux_logits": aux_logits  # (B, 28) - for L_Aux
     }
     ```

**V4 å…³é”®è®¾è®¡å˜æ›´:**

| å˜æ›´ç‚¹ | V3 (æ—§) | V4 (æ–°) |
|--------|---------|---------|
| æŠ•å½±ä½ç½® | å…ˆå– [CLS] tokenï¼Œå†æŠ•å½± | å…ˆæŠ•å½±æ•´ä¸ªåºåˆ—ï¼Œå† Pooling |
| Branch A è¾“å…¥ | `cls_emb` (B, 768) | `last_hidden` (B, L, 768) |
| Branch A è¾“å‡º | `mu` (B, 64) ç›´æ¥ | `semantic_seq` (B, L, 64) â†’ Pooling |
| Branch B æœºåˆ¶ | Gravitational Attention (åŒå‘) | **Max Pooling** (ä¸“æ³¨åº¦æ˜¯å¼ºåº¦é‡) |
| Pooling æ–¹å¼ | æ—  (ç›´æ¥ç”¨ [CLS]) | Masked Mean (A) / **Max** (B) |
| å®ä½“æ”¯æŒ | âŒ ä»…æ”¯æŒæ•´å¥ | âœ… æ”¯æŒ Token ç²’åº¦å®ä½“ |

**Masking ç¤ºä¾‹:**

```python
# æ•´å¥æ¨¡å¼ (entity_mask=None)
text = "I am absolutely furious right now!"
# attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] (å…¨ 1)
# pool_mask:     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] (å…¨å¥)

# å®ä½“æ¨¡å¼ (entity_mask æä¾›)
text = "The movie was fantastic but acting terrible"
# entity_mask:   [0, 0, 0, 1, 1, 1, 1, 0, 0, 0] (ä»… "fantastic")
# pool_mask:     [0, 0, 0, 1, 1, 1, 1, 0, 0, 0] (ä»…å¯¹å®ä½“ pooling)
```

**Branch C ç›‘ç£ä½œç”¨:** Branch C ä¾ç„¶æ¥åœ¨ **Pooling åçš„ mu** ä¸Šï¼Œå¯¹æœ€ç»ˆæå–çš„å®ä½“/å¥å­å‘é‡è¿›è¡Œè¯­ä¹‰ç›‘ç£ã€‚å¦‚æœ Pooling ä¸¢å¤±äº†å…³é”®ä¿¡æ¯ï¼ŒBranch C çš„ KL æŸå¤±ä¼šæƒ©ç½šæ¨¡å‹ã€‚

### 4.4 `src/loss.py` (æŸå¤±å‡½æ•°) - **Supervised vMF-NCE**

å®ç° Supervised vMF-NCE å¯¹æ¯”å­¦ä¹  + Calibration + Auxiliary Loss + **å®ä½“çº§ç›‘ç£**ã€‚

**æ ¸å¿ƒå˜æ›´ï¼šæ”¾å¼ƒ SimCSEï¼Œé‡‡ç”¨ç›‘ç£ä¿¡å·**

SimCSE çš„é—®é¢˜ï¼šå°†åŒç±»æƒ…æ„Ÿæ ·æœ¬ï¼ˆå¦‚ "I love cats" å’Œ "I adore dogs"ï¼‰è§†ä¸ºè´Ÿä¾‹ï¼Œå¼ºè¿«æ¨¡å‹å­¦ä¹ "å¥å­ç›¸ä¼¼åº¦"è€Œé"æƒ…æ„Ÿç›¸ä¼¼åº¦"ã€‚

**Solution:** Class-Prototype Supervised vMF-NCE - æ‹‰è¿‘æ ·æœ¬ä¸å…¶æƒ…æ„Ÿç±»åˆ«ä¸­å¿ƒçš„è·ç¦»ã€‚

* **ç±»:** `GBERTLoss(nn.Module)`
* **`__init__(config, num_emotions=28, embedding_dim=64)`:**
  * ä¿å­˜æƒé‡ `lambda_cal`, `lambda_aux`
  * **åˆ›å»º Class Prototypes:** `self.prototypes = nn.Parameter(torch.randn(num_emotions, embedding_dim))`
    * å¯å­¦ä¹ å‚æ•°ï¼š28 ä¸ªæƒ…æ„Ÿç±»åˆ«çš„ä¸­å¿ƒå‘é‡
    * åˆå§‹åŒ–ä¸º L2 å½’ä¸€åŒ–çš„éšæœºå‘é‡

* **`forward(outputs, soft_labels)`:**

  **Single Forward (ä¸å†éœ€è¦åŒå‰å‘):**
  - `outputs`: å•æ¬¡å‰å‘ä¼ æ’­çš„è¾“å‡º
  - `soft_labels`: (B, 28) Soft Label åˆ†å¸ƒ

  1. **Extract:** è·å– `mu`, `kappa`, `aux_logits`ã€‚

  2. **Calc $L_{vMF}$ (Supervised Class-Prototype ä¸»æŸå¤±):**
     ```python
     # Step A: å½’ä¸€åŒ– Prototypes (ç¡®ä¿åœ¨è¶…çƒé¢ä¸Š)
     prototypes_norm = F.normalize(self.prototypes, p=2, dim=1)

     # Step B: è®¡ç®— Cosine Similarity
     # mu: (B, 64), prototypes_norm: (28, 64) -> logits: (B, 28)
     logits = torch.matmul(mu, prototypes_norm.T)

     # Step C: åŠ¨æ€æ¸©åº¦ (å…³é”®ï¼šDetach Kappa é˜²æ­¢æ¢¯åº¦å›ä¼ !)
     # L_vMF åªæ›´æ–° mu å’Œ prototypesï¼Œkappa ç”± L_Cal å•ç‹¬ä¼˜åŒ–
     tau = 1.0 / (kappa.detach() + 1e-6)  # (B, 1)
     scaled_logits = logits / tau  # (B, 28)

     # Step D: Soft Label ä½œä¸ºç›®æ ‡åˆ†å¸ƒ (å¤šæ ‡ç­¾æ”¯æŒ)
     log_probs = F.log_softmax(scaled_logits, dim=1)  # (B, 28)
     L_vMF = F.kl_div(log_probs, soft_labels, reduction='batchmean')
     ```

  3. **Calc $L_{Cal}$ (æ ¡å‡†æŸå¤±):**
     * **å…³é”®ä¿®æ­£ï¼š** æ’é™¤ Neutral ç±»åˆ«ï¼ˆindex=27ï¼‰è®¡ç®— Max-Normï¼Œé˜²æ­¢ä¸­æ€§å¥å­è·å¾—é«˜Îº
     * è®¡ç®— Target: `kappa_tgt = 1.0 + 50.0 * torch.max(soft_labels[:, :27], dim=1).values`
     * è®¡ç®— MSE Loss: `F.mse_loss(kappa.squeeze(), kappa_tgt)`

  4. **Calc $L_{Aux}$ (è¾…åŠ©æŸå¤±):**
     * è®¡ç®— KL Divergence: `F.kl_div(F.log_softmax(aux_logits, dim=1), soft_labels, reduction='batchmean')`

  5. **Sum:** `L_total = L_vMF + lambda_cal * L_Cal + lambda_aux * L_Aux`

**å…³é”®å·®å¼‚å¯¹æ¯”:**

| ç‰¹æ€§ | SimCSE (æ—§) | Supervised vMF-NCE (æ–°) |
|------|-------------|-------------------------|
| æ­£å¯¹å®šä¹‰ | åŒä¸€è¾“å…¥çš„ Dropout å˜ä½“ | åŒæƒ…æ„Ÿç±»åˆ«çš„æ‰€æœ‰æ ·æœ¬ |
| è´Ÿå¯¹å®šä¹‰ | Batch å†…å…¶ä»–æ‰€æœ‰æ ·æœ¬ | ä¸åŒæƒ…æ„Ÿç±»åˆ«çš„ä¸­å¿ƒ |
| å‰å‘æ¬¡æ•° | 2 æ¬¡ (åŒä¸€è¾“å…¥) | 1 æ¬¡ |
| Label ä½¿ç”¨ | ä»…ç”¨äº Calibration | ç›´æ¥æŒ‡å¯¼ Embedding å­¦ä¹  |
| æ”¶æ•›é€Ÿåº¦ | æ…¢ (éœ€è¦å¤§é‡æ ·æœ¬) | å¿« (æœ‰æ˜ç¡®çš„ç±»åˆ«ç›®æ ‡) |

**å…³é”®è®¾è®¡ç»†èŠ‚:**

| è®¾è®¡ç‚¹ | å®ç° | ç›®çš„ |
|--------|------|------|
| **æ¢¯åº¦æˆªæ–­** | `kappa.detach()` | ç¡®ä¿ $\kappa$ ä»…ç”± $L_{Cal}$ ä¼˜åŒ–ï¼Œé˜²æ­¢"ä½œå¼Š" |
| **åŸå‹å½’ä¸€åŒ–** | `F.normalize(self.prototypes)` | ç¡®ä¿åŸå‹åœ¨è¶…çƒé¢ä¸Šï¼Œé˜²æ­¢æ¨¡é•¿è†¨èƒ€ |

**çƒ­åŠ›å­¦è§£é‡Š (ä¿ç•™):** åŠ¨æ€æ¸©åº¦ Ï„ ä½¿ç³»ç»Ÿåœ¨ä¸åŒ"çƒ­åŠ›å­¦æ€"é—´è‡ªé€‚åº”åˆ‡æ¢ï¼š
* **Solid State (Îº â†’ 50, Ï„ â†’ 0):** ä½æ¸©ä½ç†µï¼Œé«˜ç²¾åº¦æ£€ç´¢
* **Gaseous State (Îº â†’ 1~10, Ï„ å‡é«˜):** é«˜æ¸©é«˜ç†µï¼Œé«˜æ³›åŒ–æ€§

### 4.5 `src/utils.py` (å·¥å…·å‡½æ•°)

* **`Logger`:** ç»Ÿä¸€çš„æ—¥å¿—æ¥å£
  * é»˜è®¤è¾“å‡ºåˆ° Console å’Œæ–‡ä»¶ `logs/train_{timestamp}.log`
  * æ£€æµ‹ WandB: å¦‚æœ `wandb` å·²å®‰è£…ä¸”ç™»å½•ï¼Œè‡ªåŠ¨åˆå§‹åŒ–

* **`set_seed(seed)`:** è®¾ç½®éšæœºç§å­ (torch, numpy, random)

* **`get_device()`:** åŠ¨æ€è·å–è®¾å¤‡ï¼Œæ”¯æŒ CUDA/MPS/CPU

### 4.6 `train.py` (è®­ç»ƒå¾ªç¯)

ä¸»æ§åˆ¶æµï¼ŒåŒ…å«ç¡¬ä»¶é€‚é…ã€å­¦ä¹ ç‡è°ƒåº¦ã€æ—©åœç­‰é€»è¾‘ã€‚

* **åŠŸèƒ½æµç¨‹:**

  1. **ç¡¬ä»¶æ£€æµ‹:**
  ```python
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  if torch.cuda.is_available():
      torch.backends.cudnn.benchmark = True
      logger.info(f"ğŸš€ GPU Activated: {torch.cuda.get_device_name(0)}")
  else:
      logger.warning("âš ï¸ GPU Not Found. Falling back to CPU. Training will be slow.")
  ```

  2. åˆå§‹åŒ– `Config`, `Tokenizer`, `Model`, `Loss`, `Optimizer`, `Scheduler`ã€‚
  3. **LR Scheduler:** `get_linear_schedule_with_warmup`
     - `warmup_steps = total_steps * warmup_ratio`
  4. æ¨¡å‹ `.to(device)`ã€‚
  5. **Early Stopping å˜é‡:** `best_val_loss = inf`, `patience_counter = 0`ã€‚
  6. **Loop Epochs:**
     * **Train Mode:**
       * **Loop Batches:**
         * **Single Forward (Supervised vMF-NCE):** `outputs = model(batch)` (ä¸å†éœ€è¦åŒå‰å‘)
         * è®¡ç®— Loss: `loss = criterion(outputs, batch['soft_label'])`
         * **Gradient Accumulation:**
           ```python
           loss = loss / config.grad_accum_steps
           loss.backward()
           if (step + 1) % config.grad_accum_steps == 0:
               optimizer.step()
               scheduler.step()
               optimizer.zero_grad()
           ```
         * Logging (æ¯ N æ­¥): loss components, learning rate, avg kappa
     * **Val Mode (æ¯ Epoch ç»“æŸ):**
       * è®¡ç®— Val Loss (ä¸æ›´æ–°æ¢¯åº¦)
       * è®°å½• Val_Total_Loss, Val_vMF_Loss, Val_Cal_Loss
       * æ‰“å°ä¸€ä¸ª Batch çš„ Average Kappa (ç›‘æ§è¯­ä¹‰å¡Œç¼©)
     * **Checkpoint & Early Stopping:**
       ```python
       if val_loss < best_val_loss:
           best_val_loss = val_loss
           patience_counter = 0
           torch.save(model.state_dict(), "checkpoints/best_model.pt")
       else:
           patience_counter += 1
       torch.save({
           'model': model.state_dict(),
           'optimizer': optimizer.state_dict(),
           'scheduler': scheduler.state_dict(),
           'epoch': epoch,
           'best_val_loss': best_val_loss,
       }, "checkpoints/last_checkpoint.pt")
       if patience_counter >= config.patience:
           logger.info("Early stopping triggered")
           break
       ```

### 4.7 `inference.py` (äº¤äº’å¼æ¨ç† Demo) - **V4: Entity-Aware**

**åŠŸèƒ½:** å•å¥æƒ…æ„Ÿåˆ†æ + å®ä½“çº§æƒ…æ„Ÿæå–å·¥å…·ï¼ˆæ”¯æŒ span_text è¾“å…¥ï¼‰

* **ç±»:** `GbertPredictor`
  * `load_model(checkpoint_path)`: åŠ è½½æ¨¡å‹æƒé‡
  * `predict(text, span_text=None)`: è¿”å›é¢„æµ‹ç»“æœ
    * `span_text=None`: æ•´å¥æ¨¡å¼
    * `span_text="fantastic"`: å®ä½“æ¨¡å¼ï¼ˆè‡ªåŠ¨å­—ç¬¦å¯¹é½ï¼‰

* **æ¨¡å¼ 1: æ•´å¥æ¨¡å¼ (entity_mask=None)**
  ```python
  >>> predictor = GbertPredictor.load("checkpoints/best_model.pt")
  >>> result = predictor.predict("I am absolutely furious right now!")
  >>> print(result)
  {
      "text": "I am absolutely furious right now!",
      "mode": "sentence",
      "category": "anger",           # Soft Label Top-1
      "intensity": 0.85,              # I_raw = max(soft_label)
      "kappa": 43.2,                  # ç‰©ç†è´¨é‡
      "mu": [0.12, -0.34, ...]        # 64d å‘é‡ (æ˜¾ç¤ºå‰5ç»´)
  }
  ```

* **æ¨¡å¼ 2: å®ä½“æ¨¡å¼ (entity_mask æŒ‡å®š)** - V4 æ–°å¢
  ```python
  >>> text = "The movie was fantastic but acting terrible"
  >>> # å¯¹ "fantastic" åšæƒ…æ„Ÿåˆ†æ
  >>> result = predictor.predict(
  ...     text,
  ...     span_text="fantastic"  # ä½¿ç”¨åŸå§‹æ–‡æœ¬ï¼Œè€Œé token ç´¢å¼•
  ... )
  >>> print(result)
  {
      "text": "The movie was fantastic but acting terrible",
      "mode": "entity",
      "span_text": "fantastic",
      "category": "joy",
      "intensity": 0.72,
      "kappa": 37.1,
      "mu": [0.08, 0.21, ...]        # ä»…åŸºäº "fantastic" çš„è¯­ä¹‰å‘é‡
  }
  ```

* **å†…éƒ¨å®ç° (è‡ªåŠ¨å­—ç¬¦å¯¹é½ + Empty Mask ä¿æŠ¤):**
  ```python
  def predict(self, text, span_text=None):
      """
      Args:
          text: è¾“å…¥æ–‡æœ¬
          span_text: å®ä½“æ–‡æœ¬ (å¦‚ "fantastic")ï¼Œå¦‚æœä¸º None åˆ™åˆ†ææ•´å¥
      """
      encoding = self.tokenizer(text, return_offsets_mapping=True)

      if span_text is None:
          entity_mask = None  # æ•´å¥æ¨¡å¼
      else:
          # ä½¿ç”¨ä¸ Dataset ç›¸åŒçš„å­—ç¬¦å¯¹é½é€»è¾‘
          entity_mask = self._create_entity_mask(text, span_text, encoding)

      inputs = {
          'input_ids': torch.tensor(encoding['input_ids']).unsqueeze(0),
          'attention_mask': torch.tensor(encoding['attention_mask']).unsqueeze(0),
          'entity_mask': torch.tensor(entity_mask).unsqueeze(0) if entity_mask is not None else None
      }

      # å…³é”®ä¿®æ­£ï¼ˆEmpty Mask ä¿æŠ¤ï¼‰ï¼šæ£€æŸ¥ entity_mask æ˜¯å¦å…¨ä¸º 0
      if entity_mask is not None and entity_mask.sum() == 0:
          import warnings
          warnings.warn(f"Entity '{span_text}' not found in text. Falling back to sentence-level analysis.")
          inputs['entity_mask'] = None

      with torch.no_grad():
          outputs = self.model(**inputs)

      return self._format_result(text, span_text, outputs)
  ```

---

## 5. å…³é”®å®ç°ç»†èŠ‚ (Key Implementation Details)

### 5.1 Token-Level Pooling (V4 æ ¸å¿ƒ)

V4 æ¶æ„çš„æ ¸å¿ƒåˆ›æ–°æ˜¯å°†æŠ•å½±å±‚ç§»åˆ° Backbone ä¹‹åã€Pooling ä¹‹å‰ï¼š

```python
# V3 (æ—§): [CLS] Token ç›´æ¥æŠ•å½±
cls_emb = last_hidden[:, 0, :]  # (B, 768)
mu = F.normalize(self.semantic_head(cls_emb), dim=1)

# V4 (æ–°): å…ˆæŠ•å½±æ•´ä¸ªåºåˆ—ï¼Œå† Pooling
semantic_seq = self.semantic_head(last_hidden)  # (B, L, 64)
# Masked Mean Pooling
masked = semantic_seq * pool_mask.unsqueeze(-1)  # (B, L, 64)
mu_raw = masked.sum(dim=1) / pool_mask.sum(dim=1).clamp(min=1e-9)  # (B, 64)
mu = F.normalize(mu_raw, p=2, dim=1)
```

**ä¼˜åŠ¿:**
1. **å®ä½“æ„ŸçŸ¥:** å¯ä»¥ä»…å¯¹ç‰¹å®š Token åš Poolingï¼Œæå–å®ä½“è¯­ä¹‰
2. **æ›´ä¸°å¯Œçš„è¯­ä¹‰:** ä¸å†ä¾èµ– [CLS] çš„é¢„è®¾ä½ç½®ï¼Œä½¿ç”¨å®é™…çš„è¯­ä¹‰ Token
3. **çµæ´»çš„æ³¨æ„åŠ›:** å¯ä»¥é€šè¿‡è°ƒæ•´ entity_mask å®ç°è½¯/ç¡¬æ³¨æ„åŠ›

### 5.2 Supervised vMF-NCE è®­ç»ƒæµç¨‹ (å¯¹æ¯”å­¦ä¹ æ ¸å¿ƒ)

```python
# è®­ç»ƒæ—¶ï¼Œå•æ¬¡å‰å‘ä¼ æ’­
for batch in dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}

    # Single Forward - Supervised vMF-NCE
    outputs = model(batch['input_ids'], batch['attention_mask'], batch['entity_mask'])

    # è®¡ç®—æŸå¤± (ä½¿ç”¨ Class Prototypes)
    loss = criterion(outputs, batch['soft_label'])
```

### 5.3 ç¡¬ä»¶å›é€€ç­–ç•¥ (Hardware Fallback)

åœ¨æ‰€æœ‰æ¶‰åŠ Tensor è¿ç®—çš„åœ°æ–¹ï¼Œéƒ½ä¸ç¡¬ç¼–ç  `.cuda()`ï¼Œè€Œæ˜¯ä½¿ç”¨ `config.device` æˆ– `tensor.to(device)`ã€‚

### 5.4 æ•°å€¼ç¨³å®šæ€§ (Numerical Stability)

* **Kappa ä¸‹ç•Œ:** `tau = 1.0 / (kappa + 1e-6)`
* **Softplus:** `token_energies = F.softplus(self.energy_proj(last_hidden))`

### 5.5 æ¨¡æ‹Ÿå¤§ Batch (Gradient Accumulation)

```python
loss = loss / config.grad_accum_steps  # ç¼©æ”¾ loss
loss.backward()
if (step + 1) % config.grad_accum_steps == 0:
    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()
```
**Effective Batch Size = Physical Ã— Accum = 64 Ã— 4 = 256**

### 5.7 å­—ç¬¦å¯¹é½é—®é¢˜è¯¦è§£ (V4 Critical)

**é—®é¢˜æ ¹æº:**

Tokenizer çš„ WordPiece/BPE ç®—æ³•ä¼šå°†å•è¯æ‹†åˆ†ä¸ºå¤šä¸ª Subword Tokenï¼š
```python
text = "I absolutely love it!"
tokens = tokenizer.tokenize(text)
# ['I', 'absolutely', 'love', 'it', '!']

# ä½†å¦‚æœç”¨æˆ·æ ‡æ³¨çš„æ˜¯ "absolutely" è¿™ä¸ªè¯
# åœ¨ Token åºåˆ—ä¸­çš„ä½ç½®æ˜¯å¤šå°‘ï¼Ÿç›´æ¥ç”¨å•è¯ç´¢å¼•æ˜¯é”™è¯¯çš„ï¼
```

**é”™è¯¯åšæ³• (ä¸¥ç¦):**
```python
# âŒ å‡è®¾ "absolutely" æ˜¯ç¬¬ 2 ä¸ªå•è¯ï¼Œå°±è®¤ä¸º token_idx = 1
# è¿™åœ¨ Subword Tokenization ä¸‹å®Œå…¨é”™è¯¯ï¼
```

**æ­£ç¡®åšæ³• (offset_mapping):**
```python
# âœ… ä½¿ç”¨å­—ç¬¦çº§å¯¹é½
encoding = tokenizer(text, return_offsets_mapping=True)
# offset_mapping = [(0, 1), (2, 12), (13, 17), (18, 20), (20, 21)]
#                    'I'  'absolutely'  'love'   'it'     '!'

# ç”¨æˆ·æ ‡æ³¨ "absolutely" (char_start=2, char_end=12)
# éå† offset_mappingï¼Œæ‰¾åˆ°å®Œå…¨åŒ…å«åœ¨ [2, 12) åŒºé—´å†…çš„ Token
span_text = "absolutely"
# æ•°æ®ä¸­å·²æä¾›: char_start=2, char_end=12 (V4: æ˜¾å¼ offsetï¼Œæ— éœ€ text.find())
c_start, c_end = item['char_start'], item['char_end']

# V4: ä½¿ç”¨å‘é‡åŒ–æ“ä½œè®¡ç®— entity_mask
token_starts = offsets[:, 0]
token_ends = offsets[:, 1]

# Token ä¸ Entity æœ‰äº¤é›† â†’ True
entity_mask = (token_starts < c_end) & (token_ends > c_start) & attention_mask.bool()
# ç»“æœ: [0, 1, 0, 0, 0] - åªæœ‰ "absolutely" è¢«æ ‡è®°
```

**è¾¹ç•Œæƒ…å†µå¤„ç†:**

1. **éƒ¨åˆ†é‡å :** ä½¿ç”¨äº¤é›†åˆ¤æ–­ (`<` and `>`)ï¼Œè€Œéå®Œå…¨åŒ…å«
2. **é‡å¤è¯é—®é¢˜:** æ˜¾å¼ offset ç²¾ç¡®å®šä½ï¼Œæ— æ­§ä¹‰ï¼ˆV4 å…³é”®æ”¹è¿›ï¼‰

**V4 å…³é”®æ”¹è¿›ï¼š** æ•°æ®ä¸­æ˜¾å¼åŒ…å« `char_start` å’Œ `char_end`ï¼Œä¸å†ä½¿ç”¨ `text.find()`ï¼Œé¿å…äº†é‡å¤è¯æ—¶çš„æ­§ä¹‰é—®é¢˜ã€‚

**Subword ç¤ºä¾‹:**
```python
text = "I love transformers"
# Tokenizer: ['I', 'love', 'transform', '##ers']
# offset_mapping: [(0,1), (2,6), (7,16), (16,20)]

# æ ‡æ³¨ "transformers"
char_start = 7
char_end = 19
# å¯¹é½ç»“æœ: [0, 0, 1, 1] - "transform" + "##ers" ä¸¤ä¸ª Token
```

### 5.8 GoEmotions æ•°æ®å¤„ç†ç¤ºä¾‹ (Legacy)

```python
# å‡è®¾åŸå§‹æ ¼å¼
# {
#   "text": "I'm so happy!",
#   "labels": [0, 0, 5, 1, 0, ...]  # 28 ç±»åˆ«ï¼Œæ¯ä¸ªæ˜¯æ ‡æ³¨äººæ•°
# }

# å½’ä¸€åŒ–ä¸º Soft Label
import torch
labels = torch.tensor([0, 0, 5, 1, 0, ...], dtype=torch.float32)
soft_label = labels / labels.sum()  # å½’ä¸€åŒ–
intensity = torch.max(soft_label).item()  # Max-Norm
```

---

## 6. å¼€å‘æ­¥éª¤ (Action Plan)

1. **ç¯å¢ƒå‡†å¤‡:** åˆ›å»º `requirements.txt` (torch, transformers, scikit-learn, tqdm, wandb[optional])ã€‚
2. **æ•°æ®å‡†å¤‡:** ä½¿ç”¨ GoEmotions Raw Annotations ç”Ÿæˆ `train.jsonl`ï¼ˆå½’ä¸€åŒ–æŠ•ç¥¨æ•°ï¼‰ã€‚
3. **æ ¸å¿ƒæ¨¡å—:** ä¾æ¬¡å®ç° `config.py` â†’ `model.py` â†’ `loss.py` â†’ `dataset.py` â†’ `utils.py`ã€‚
4. **è®­ç»ƒè„šæœ¬:** ç¼–å†™ `train.py`ï¼Œå…ˆç”¨ CPU åœ¨å°‘é‡æ•°æ®ä¸Šè·‘é€šæµç¨‹ (Debug mode)ã€‚
5. **æ¨ç†è„šæœ¬:** ç¼–å†™ `inference.py` äº¤äº’å¼ Demoã€‚
6. **å…¨é‡è®­ç»ƒ:** åˆ‡æ¢åˆ° GPU è¿›è¡Œå®Œæ•´è®­ç»ƒã€‚

---

## 7. ä¾èµ–åŒ… (Requirements)

```
torch>=2.0.0
transformers>=4.30.0
scikit-learn
tqdm
wandb  # Optional, ç”¨äºå®éªŒè·Ÿè¸ª
openai  # Optional, ç”¨äº generate_data.py
datasets  # GoEmotions æ•°æ®é›†åŠ è½½
```
