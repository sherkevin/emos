# Probabilistic G-BERT: System Architecture V3 (NIPS Edition)

## Abstract

ç°æœ‰çš„æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼ˆå¦‚ BERT, E5ï¼‰é€šå¸¸å°†è¯­ä¹‰æ˜ å°„ä¸ºæ¬§æ°ç©ºé—´ä¸­çš„**ç¡®å®šæ€§ç‚¹å‘é‡**ã€‚è¿™ç§**å„å‘åŒæ€§ï¼ˆIsotropicï¼‰**çš„å‡è®¾å¿½ç•¥äº†äººç±»è®°å¿†ä¸æƒ…æ„Ÿçš„ä¸€ä¸ªæ ¸å¿ƒç‰¹å¾ï¼š**"å¼ºåº¦å³ç¡®å®šæ€§" (Intensity implies Certainty)**ã€‚

ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡º **Probabilistic G-BERT**ã€‚ä¸åŒäºå­¦ä¹ é™æ€å‘é‡ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªæ–‡æœ¬å»ºæ¨¡ä¸ºè¶…çƒé¢ä¸Šçš„ **Von Mises-Fisher (vMF)** æ¦‚ç‡åˆ†å¸ƒã€‚

1. **ç†è®ºåŸºç¡€ï¼š** ä»æ¬§æ°ç©ºé—´çš„ç‚¹å‡è®¾è½¬å‘è¶…çƒé¢ä¸Šçš„**æ¦‚ç‡åˆ†å¸ƒ**å‡è®¾ã€‚
2. **æ•°æ®ç­–ç•¥ï¼š** å¼•å…¥ **Soft Label Max-Norm**ï¼Œå°†"å¼ºåº¦"å®šä¹‰ä¸ºæ ‡ç­¾åˆ†å¸ƒçš„å‡ ä½•å°–é”ç¨‹åº¦ã€‚
3. **æ¶æ„åˆ›æ–°ï¼š** è®¾è®¡äº† **Bottlenecked Tri-Branch** ç½‘ç»œï¼ŒåŒæ—¶å­¦ä¹ è¯­ä¹‰æ–¹å‘ã€ç‰©ç†è´¨é‡å’Œè¾…åŠ©è¯­ä¹‰çº¦æŸã€‚
4. **è®­ç»ƒæœºåˆ¶ï¼š** æå‡ºäº† **Mass-Adaptive Temperature Scaling (MATS)**ï¼Œä» vMF æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ¨å¯¼åŠ¨æ€å¯¹æ¯”å­¦ä¹ æ¸©åº¦ã€‚

---

## 1. Theoretical Formulation (ç†è®ºå½¢å¼åŒ–)

### 1.1 ä»ç‚¹å‘é‡åˆ°æ¦‚ç‡åˆ†å¸ƒ

ä¼ ç»ŸåµŒå…¥æ¨¡å‹å‡è®¾æ–‡æœ¬ $x$ çš„è¡¨å¾æ˜¯ä¸€ä¸ªç¡®å®šæ€§å‘é‡ï¼š
$$\mathbf{z} = f_{\theta}(x) \in \mathbb{R}^d$$

æˆ‘ä»¬**æ”¾å¼ƒè¿™ä¸€å‡è®¾**ï¼Œè½¬è€Œå‡è®¾æ–‡æœ¬çš„æ½œåœ¨è¡¨å¾æœä» **Von Mises-Fisher (vMF) åˆ†å¸ƒ**ï¼š

$$p(\mathbf{z}|x) = C_d(\kappa) \cdot \exp(\kappa \cdot \boldsymbol{\mu}^\top \mathbf{z})$$

å…¶ä¸­ï¼š

| å‚æ•° | ç¬¦å· | ç‰©ç†æ„ä¹‰ | ç»Ÿè®¡æ„ä¹‰ |
|------|------|----------|----------|
| å‡å€¼æ–¹å‘ | $\boldsymbol{\mu}$ | è¯­ä¹‰æ ¸å¿ƒ | åˆ†å¸ƒçš„ä¸­å¿ƒæ–¹å‘ ($\|\boldsymbol{\mu}\| = 1$) |
| ä¸“æ³¨åº¦ | $\kappa$ | ç‰©ç†è´¨é‡ | åˆ†å¸ƒçš„å°–é”ç¨‹åº¦ï¼ˆç¡®å®šæ€§ï¼‰ |
| éšå˜é‡ | $\mathbf{z}$ | è¡¨å¾å‘é‡ | ä»åˆ†å¸ƒä¸­é‡‡æ ·çš„æ ·æœ¬ |

$$C_d(\kappa) = \frac{\kappa^{d/2-1}}{(2\pi)^{d/2} I_{d/2-1}(\kappa)}$$

ä¸ºå½’ä¸€åŒ–å¸¸æ•°ï¼Œ$I_v(\cdot)$ ä¸ºä¿®æ­£è´å¡å°”å‡½æ•°ã€‚

### 1.2 ç‰©ç†æ˜ å°„ï¼šè´¨é‡ä¸ä¸“æ³¨åº¦çš„ç­‰ä»·æ€§

æˆ‘ä»¬å°†è®¤çŸ¥å¿ƒç†å­¦ä¸­çš„"è®°å¿†å¼ºåº¦"æ˜ å°„ä¸ºç»Ÿè®¡å­¦ä¸­çš„"ä¸“æ³¨åº¦"ï¼š

$$\text{Intensity}(x) \;\longleftrightarrow\; \kappa(x)$$

**æ ¸å¿ƒæ´å¯Ÿï¼š**
- **å¼ºæƒ…ç»ª/é«˜ç¡®å®šæ€§** $\to$ $\kappa$ å¤§ $\to$ åˆ†å¸ƒå°–é” $\to$ æ–¹å·®å°
- **å¼±æƒ…ç»ª/ä½ç¡®å®šæ€§** $\to$ $\kappa$ å° $\to$ åˆ†å¸ƒå¹³å¦ $\to$ æ–¹å·®å¤§

å½“ $\kappa \to \infty$ï¼ŒvMF åˆ†å¸ƒåç¼©ä¸ºç‹„æ‹‰å…‹ delta å‡½æ•°ï¼ˆç‚¹å‘é‡ï¼‰ã€‚
å½“ $\kappa \to 0$ï¼ŒvMF åˆ†å¸ƒé€€åŒ–ä¸ºå‡åŒ€åˆ†å¸ƒï¼ˆå®Œå…¨æ¨¡ç³Šï¼‰ã€‚

---

## 2. Data Strategy: Soft Label Max-Norm (æ•°æ®ç­–ç•¥)

### 2.1 æ ¸å¿ƒå‡è®¾ï¼šå¼ºåº¦å³ç¡®å®šæ€§

æˆ‘ä»¬**ä¸å†ä½¿ç”¨ GPT-4 ç”Ÿæˆä¸»è§‚åˆ†æ•°**ï¼ˆå¦‚ "Score 0.9"ï¼‰ï¼Œè€Œæ˜¯è®©æ¨¡å‹å­¦ä¹ æ ‡ç­¾åˆ†å¸ƒçš„**å‡ ä½•å°–é”ç¨‹åº¦**ã€‚

**å®šä¹‰ï¼š** æƒ…ç»ªå¼ºåº¦ $I_{raw}$ ä¸º Soft Label åˆ†å¸ƒçš„**æ— ç©·èŒƒæ•°**ï¼ˆæœ€å¤§å€¼ï¼‰ï¼š

$$I_{raw} = \|\mathbf{y}\|_\infty = \max(\text{Soft\_Labels})$$

å…¶ä¸­ $\mathbf{y} \in \mathbb{R}^{28}$ æ˜¯å½’ä¸€åŒ–çš„æƒ…ç»ªç±»åˆ«æ¦‚ç‡åˆ†å¸ƒï¼Œ$\sum_{i=1}^{28} y_i = 1$ã€‚

### 2.2 å‡ ä½•è§£é‡Š

| $I_{raw}$ å€¼åŸŸ | åˆ†å¸ƒå½¢æ€ | è®¤çŸ¥è§£é‡Š | ç¤ºä¾‹ |
|----------------|----------|----------|------|
| $\to 1.0$ | æåº¦å°–é”ï¼ˆå•å³°çªå‡ºï¼‰ | é«˜ç¡®å®šæ€§ï¼Œæƒ…ç»ªæ˜ç¡® | "I am furious!" |
| $\approx 0.5$ | ä¸­ç­‰å°–é” | ä¸­ç­‰ç¡®å®šæ€§ | "I'm annoyed" |
| $\to 0.04$ | æ¥è¿‘å‡åŒ€ï¼ˆ$1/28$ï¼‰ | ä½ç¡®å®šæ€§ï¼Œæ¨¡æ£±ä¸¤å¯ | "I don't know how I feel" |

### 2.3 æ•°æ®ç”Ÿæˆ Prompt

ä½¿ç”¨ä»¥ä¸‹ LLM Prompt ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼š

```
You are an emotion analysis expert. Analyze the emotional content of the following text.

Text: "{text}"

Task: Distribute exactly 1.0 probability mass across the 28 emotion categories below.

EMOTIONS = [
    # Positive
    "admiration", "amusement", "approval", "caring", "desire",
    "excitement", "gratitude", "joy", "love", "optimism",
    "pride", "relief",

    # Negative
    "anger", "annoyance", "disappointment", "disapproval", "disgust",
    "embarrassment", "fear", "grief", "nervousness", "remorse",
    "sadness",

    # Ambiguous / Cognitive
    "confusion", "curiosity", "realization", "surprise",

    # Neutral
    "neutral"
]

Output Format (JSON only):
{
  "admiration": 0.05,
  "amusement": 0.0,
  "anger": 0.75,
  "annoyance": 0.10,
  ...
  "neutral": 0.05
}

Ensure all values are non-negative and sum to exactly 1.0.
```

### 2.4 æ•°æ®æ ¼å¼

```json
{
  "text": "I am absolutely furious right now!",
  "soft_label": [0.01, 0.02, 0.85, 0.05, ...],  // 28-dim probability vector
  "intensity": 0.85  // Max-Norm, computed as max(soft_label)
}
```

---

## 3. Neural Architecture: Bottlenecked Tri-Branch (æ¨¡å‹æ¶æ„)

### 3.1 æ¶æ„æ¦‚è§ˆ

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚        RoBERTa-base (768d)          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                                     â”‚
                    â–¼                                     â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Branch A    â”‚                     â”‚   Branch B    â”‚
            â”‚  (Semantic)   â”‚                     â”‚    (Mass)     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                                     â”‚
                    â–¼                                     â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚       Î¼       â”‚                     â”‚       Îº       â”‚
            â”‚     (64d)     â”‚                     â”‚     (1d)      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Branch C    â”‚
            â”‚  (Auxiliary)  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚    logits     â”‚
            â”‚     (28d)     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Branch A: Semantic Core (è¯­ä¹‰æ ¸å¿ƒ)

**ç›®æ ‡ï¼š** æå– vMF åˆ†å¸ƒçš„å‡å€¼æ–¹å‘ $\boldsymbol{\mu}$

**è®¾è®¡åŸç†ï¼š** åŸºäº **Information Bottleneck (IB)** ç†è®ºï¼Œé€šè¿‡å¼ºåˆ¶é™ç»´è¿«ä½¿æ¨¡å‹ä¸¢å¼ƒå¥æ³•å™ªå£°ï¼Œåªä¿ç•™æ ¸å¿ƒè¯­ä¹‰ã€‚

**ç½‘ç»œç»“æ„ï¼š**

```
[CLS] Token (768d)
    â”‚
    â–¼
Linear(768 â†’ 256)
    â”‚
    â–¼
GELU Activation
    â”‚
    â–¼
Linear(256 â†’ 64)      â† Information Bottleneck
    â”‚
    â–¼
L2 Normalization
    â”‚
    â–¼
Î¼ âˆˆ ğ‘†â¶Â³ (64d unit vector)
```

**å…³é”®ç‚¹ï¼š**
- è¾“å‡ºç»´åº¦ä¸¥æ ¼é™åˆ¶ä¸º **64ç»´**
- è¾“å‡ºå¿…é¡» L2 å½’ä¸€åŒ–ï¼š$\|\boldsymbol{\mu}\| = 1$

### 3.3 Branch B: Physical Mass (ç‰©ç†è´¨é‡)

**ç›®æ ‡ï¼š** é¢„æµ‹ vMF åˆ†å¸ƒçš„ä¸“æ³¨åº¦å‚æ•° $\kappa$

**è®¾è®¡åŸç†ï¼š** é€šè¿‡ **Gravitational Attention** æœºåˆ¶ï¼Œèšåˆ Token çº§åˆ«çš„èƒ½é‡ï¼Œæ¨¡æ‹Ÿç‰©ç†è´¨é‡çš„å½¢æˆè¿‡ç¨‹ã€‚

**ç½‘ç»œç»“æ„ï¼š**

```
Last Hidden States (B, L, 768)
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Energy  â”‚     â”‚ Attn    â”‚
â”‚ Proj    â”‚     â”‚ Proj    â”‚
â”‚(768â†’1)  â”‚     â”‚(768â†’1)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                 â”‚
    â–¼                 â–¼
Softplus(e_i)    Softmax(Î±_i)    + Mask(padding)
    â”‚                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
              Weighted Sum: Î£ Î±_i Â· e_i
                      â”‚
                      â–¼
                  mass_raw
                      â”‚
                      â–¼
         Îº = 1.0 + Î± Ã— mass_raw
         (Î± = 50.0, see scaling theory)
```

**ç‰©ç†ç¼©æ”¾å‚æ•° $\alpha = 50.0$ çš„ç†è®ºä¾æ®ï¼š**

å½“ $I_{raw} \approx 1.0$ï¼ˆæœ€é«˜ç¡®å®šæ€§ï¼‰æ—¶ï¼š
- $\kappa_{target} = 1.0 + 50.0 \times 1.0 = 51$
- å¯¹æ¯”å­¦ä¹ æ¸©åº¦ $\tau = 1/\kappa \approx 0.0196 \approx 0.02$

æ ¹æ® InfoNCE çš„ç†è®ºåˆ†æï¼Œ$\tau \approx 0.02$ æ˜¯**åŒºåˆ†éš¾è´Ÿæ ·æœ¬çš„æœ€ä½³ä½æ¸©åŒºé—´**ï¼Œç¡®ä¿å¼ºæƒ…ç»ªæ ·æœ¬å…·æœ‰é«˜ç‰¹å¼‚æ€§ã€‚

#### Thermodynamic Interpretation (çƒ­åŠ›å­¦è§£é‡Š)

MATS æœºåˆ¶èµ‹äºˆäº†å¯¹æ¯”å­¦ä¹ æ¸©åº¦ $\tau$ æ˜ç¡®çš„ç‰©ç†æ„ä¹‰ï¼Œè€Œéä»…ä½œä¸ºä¸€ä¸ªè¶…å‚æ•°ï¼š

| çƒ­åŠ›å­¦æ€ | $I_{raw}$ èŒƒå›´ | $\kappa$ å€¼ | $\tau$ å€¼ | ç³»ç»Ÿè¡Œä¸º |
|----------|---------------|-------------|-----------|----------|
| **Solid State** (å›ºæ€/ç»“æ™¶æ€) | $\to 1.0$ | æå¤§ ($\approx 50$) | $\to 0$ | åˆ†å¸ƒæåº¦å°–é”ï¼Œç±»ä¼¼äºæ™¶ä½“ç»“æ„ã€‚ç³»ç»Ÿå¤„äº"ä½æ¸©ä½ç†µ"çŠ¶æ€ï¼Œä»…å…è®¸è¯­ä¹‰å®Œå…¨ä¸€è‡´çš„æ ·æœ¬åŒ¹é…ï¼Œå®ç°é«˜ç²¾åº¦æ£€ç´¢ã€‚ |
| **Gaseous State** (æ°”æ€/é«˜ç†µæ€) | $\to 0$ | è¾ƒå° ($\approx 1\text{--}10$) | å‡é«˜ | åˆ†å¸ƒå¹³å¦ã€‚ç³»ç»Ÿå¤„äº"é«˜æ¸©é«˜ç†µ"çŠ¶æ€ï¼Œå®¹å¿è¾ƒå¤§çš„è¯­ä¹‰è·ç¦»ï¼Œå…è®¸æ¨¡ç³ŠåŒ¹é…ï¼Œå®ç°é«˜æ³›åŒ–æ€§ã€‚ |

è¿™ç§æœºåˆ¶è‡ªé€‚åº”åœ°è§£å†³äº†**"Granularity-Specificity Trade-off"**ï¼ˆç²’åº¦-ç‰¹å¼‚æ€§æƒè¡¡ï¼‰éš¾é¢˜ï¼šå¼ºæƒ…ç»ªæŸ¥è¯¢è‡ªåŠ¨æ”¶ç¼©åŒ¹é…åŠå¾„ï¼Œå¼±æƒ…ç»ªæŸ¥è¯¢è‡ªåŠ¨æ‰©å±•åŒ¹é…åŠå¾„ã€‚

### 3.4 Branch C: Auxiliary Semantic Head (è¾…åŠ©è¯­ä¹‰å¤´)

**ç›®æ ‡ï¼š** é˜²æ­¢ 64ç»´ç“¶é¢ˆå±‚åœ¨è®­ç»ƒåˆæœŸå‘ç”Ÿ**è¯­ä¹‰åå¡Œ** (Semantic Collapse)

**é—®é¢˜ï¼š** ä»…ä½¿ç”¨å¯¹æ¯”å­¦ä¹ æŸå¤±æ—¶ï¼Œç“¶é¢ˆå‘é‡å¯èƒ½ä¸¢å¤±ç»†ç²’åº¦çš„ç±»åˆ«ä¿¡æ¯ã€‚

**è§£å†³æ–¹æ¡ˆï¼š** é€šè¿‡ KL æ•£åº¦çº¦æŸï¼Œå¼ºåˆ¶ $\boldsymbol{\mu}$ ä¿ç•™å¯æ¢å¤çš„æƒ…ç»ªç±»åˆ«ä¿¡æ¯ã€‚

**ç½‘ç»œç»“æ„ï¼š**

```
Î¼ (64d)
    â”‚
    â–¼
Linear(64 â†’ 128)
    â”‚
    â–¼
GELU Activation
    â”‚
    â–¼
Linear(128 â†’ 28)
    â”‚
    â–¼
aux_logits (28d)
```

**ä»…åœ¨è®­ç»ƒæ—¶ä½¿ç”¨**ï¼Œæ¨ç†æ—¶å¯ä¸¢å¼ƒã€‚

---

## 4. Training Objectives: Three-Part Loss (è®­ç»ƒç›®æ ‡)

### 4.1 æ€»æŸå¤±å‡½æ•°

$$L_{Total} = L_{vMF} + \lambda_{Cal} \cdot L_{Cal} + \lambda_{Aux} \cdot L_{Aux}$$

æ¨èè¶…å‚æ•°ï¼š$\lambda_{Cal} = 0.1$, $\lambda_{Aux} = 0.05$

### 4.2 vMF-NCE Loss ($L_{vMF}$): ä¸»æŸå¤±

**åŸç†ï¼š** åŸºäº vMF åˆ†å¸ƒå‡è®¾çš„ InfoNCE å¯¹æ¯”å­¦ä¹ 

$$L_{vMF} = -\log \frac{\exp(\boldsymbol{\mu}_i^\top \boldsymbol{\mu}_{+} / \tau_i)}{\sum_{k=1}^{N} \exp(\boldsymbol{\mu}_i^\top \boldsymbol{\mu}_k / \tau_i)}$$

å…¶ä¸­**åŠ¨æ€æ¸©åº¦**å®šä¹‰ä¸ºï¼š

$$\tau_i = \frac{1}{\kappa_i} = \frac{1}{1.0 + 50.0 \times I_{raw}^{(i)}}$$

**PyTorch å®ç°ï¼š**

```python
def vmf_nce_loss(mu, kappa, labels):
    """
    Args:
        mu: (B, 64) L2-normalized semantic directions
        kappa: (B, 1) concentration parameters
        labels: (B,) positive sample indices
    """
    # Compute cosine similarity matrix
    logits = torch.matmul(mu, mu.T)  # (B, B)

    # Dynamic temperature: tau = 1 / kappa
    tau = 1.0 / (kappa + 1e-6)  # (B, 1)

    # Apply MATS: scale logits by concentration
    scaled_logits = logits / tau  # (B, B) / (B, 1) â†’ (B, B)

    # Standard cross-entropy
    loss = F.cross_entropy(scaled_logits, labels)
    return loss
```

### 4.3 Calibration Loss ($L_{Cal}$): æ ¡å‡†æŸå¤±

**ç›®æ ‡ï¼š** ç¡®ä¿é¢„æµ‹çš„ $\kappa_{pred}$ ä¸ Soft Label çš„ Max-Norm å¼ºåº¦ä¸€è‡´ã€‚

$$L_{Cal} = \text{MSE}\left(\kappa_{pred}, \kappa_{target}\right)$$

å…¶ä¸­ç›®æ ‡å€¼ä¸ºï¼š

$$\kappa_{target} = 1.0 + 50.0 \times \max(\mathbf{y}_{soft})$$

**PyTorch å®ç°ï¼š**

```python
def calibration_loss(predicted_kappa, soft_labels):
    """
    Args:
        predicted_kappa: (B, 1) model output
        soft_labels: (B, 28) ground-truth probability distributions
    """
    # Intensity as Max-Norm of Soft Label
    I_raw = torch.max(soft_labels, dim=1).values  # (B,)

    # Target: Îº = 1.0 + 50.0 Ã— I_raw
    target_kappa = 1.0 + 50.0 * I_raw  # (B,)

    # MSE loss
    loss = F.mse_loss(predicted_kappa.squeeze(), target_kappa)
    return loss
```

### 4.4 Auxiliary Loss ($L_{Aux}$): è¾…åŠ©æŸå¤±

**ç›®æ ‡ï¼š** ç¡®ä¿ç“¶é¢ˆå‘é‡ $\boldsymbol{\mu}$ ä¿ç•™æƒ…ç»ªç±»åˆ«ä¿¡æ¯ã€‚

$$L_{Aux} = D_{KL}\left(\text{Softmax}(\text{BranchC}(\boldsymbol{\mu})) \;\|\; \mathbf{y}_{soft}\right)$$

**PyTorch å®ç°ï¼š**

```python
def auxiliary_loss(aux_logits, soft_labels):
    """
    Args:
        aux_logits: (B, 28) raw output from Branch C
        soft_labels: (B, 28) ground-truth probability distributions
    """
    log_pred = F.log_softmax(aux_logits, dim=1)
    loss = F.kl_div(log_pred, soft_labels, reduction='batchmean')
    return loss
```

---

## 5. Complete PyTorch Implementation

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModel


class ProbabilisticGBERT(nn.Module):
    """
    Probabilistic G-BERT: vMF Distribution for Text Embedding

    Architecture: Bottlenecked Tri-Branch
    - Branch A: Semantic Core (64d unit vector)
    - Branch B: Physical Mass (concentration Îº)
    - Branch C: Auxiliary Semantic Head (28d logits)
    """

    def __init__(self, model_name='roberta-base', alpha_scale=50.0):
        super().__init__()
        self.backbone = AutoModel.from_pretrained(model_name)
        hidden_size = 768

        # --- Branch A: Semantic Bottleneck (768 â†’ 64) ---
        self.semantic_head = nn.Sequential(
            nn.Linear(hidden_size, 256),
            nn.GELU(),
            nn.Linear(256, 64)
        )

        # --- Branch B: Gravitational Attention ---
        self.energy_proj = nn.Linear(hidden_size, 1)      # Token energy
        self.attn_proj = nn.Linear(hidden_size, 1)        # Attention weights
        self.alpha_scale = alpha_scale                    # Scaling factor

        # --- Branch C: Auxiliary Semantic Head (64 â†’ 28) ---
        self.aux_head = nn.Sequential(
            nn.Linear(64, 128),
            nn.GELU(),
            nn.Linear(128, 28)
        )

    def forward(self, input_ids, attention_mask):
        # Backbone encoding
        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)
        last_hidden = outputs.last_hidden_state  # (B, L, 768)
        cls_emb = last_hidden[:, 0, :]           # (B, 768)

        # Branch A: Semantic Direction Î¼
        raw_vec = self.semantic_head(cls_emb)
        mu = F.normalize(raw_vec, p=2, dim=1)    # (B, 64), ||Î¼|| = 1

        # Branch B: Concentration Îº via Gravitational Attention
        token_energies = F.softplus(self.energy_proj(last_hidden))  # (B, L, 1)
        attn_scores = self.attn_proj(last_hidden)                   # (B, L, 1)

        # Mask padding tokens
        mask = attention_mask.unsqueeze(-1)                          # (B, L, 1)
        attn_scores = attn_scores.masked_fill(mask == 0, -1e9)
        attn_weights = F.softmax(attn_scores, dim=1)                # (B, L, 1)

        # Aggregated mass
        mass = torch.sum(attn_weights * token_energies, dim=1)      # (B, 1)

        # Physical scaling: Îº = 1.0 + Î± Ã— mass
        kappa = 1.0 + self.alpha_scale * mass                       # (B, 1)

        # Branch C: Auxiliary logits
        aux_logits = self.aux_head(mu)                              # (B, 28)

        return {
            "mu": mu,              # (B, 64) - semantic direction
            "kappa": kappa,        # (B, 1)  - concentration
            "mass": mass,          # (B, 1)  - for visualization
            "aux_logits": aux_logits  # (B, 28) - for L_Aux
        }


def vmf_nce_loss(mu, kappa, labels):
    """vMF-NCE Loss with adaptive temperature."""
    logits = torch.matmul(mu, mu.T)               # (B, B)
    tau = 1.0 / (kappa + 1e-6)                    # (B, 1)
    scaled_logits = logits / tau                  # (B, B)
    return F.cross_entropy(scaled_logits, labels)


def calibration_loss(predicted_kappa, soft_labels):
    """Calibration Loss: align Îº with Soft Label Max-Norm."""
    I_raw = torch.max(soft_labels, dim=1).values           # (B,)
    target_kappa = 1.0 + 50.0 * I_raw                      # (B,)
    return F.mse_loss(predicted_kappa.squeeze(), target_kappa)


def auxiliary_loss(aux_logits, soft_labels):
    """Auxiliary Loss: KL divergence for semantic preservation."""
    log_pred = F.log_softmax(aux_logits, dim=1)
    return F.kl_div(log_pred, soft_labels, reduction='batchmean')


def total_loss(outputs, soft_labels, labels, lambda_cal=0.1, lambda_aux=0.05):
    """Total Loss: L_vMF + Î»_Cal * L_Cal + Î»_Aux * L_Aux"""
    l_vmf = vmf_nce_loss(outputs['mu'], outputs['kappa'], labels)
    l_cal = calibration_loss(outputs['kappa'], soft_labels)
    l_aux = auxiliary_loss(outputs['aux_logits'], soft_labels)

    return l_vmf + lambda_cal * l_cal + lambda_aux * l_aux
```

---

## 6. Training Pipeline (è®­ç»ƒæµç¨‹)

### 6.1 æ•°æ®å‡†å¤‡

```python
class EmotionDataset(torch.utils.data.Dataset):
    def __init__(self, data_path):
        # Load JSONL with fields: text, soft_label (28-dim list)
        self.data = [json.loads(line) for line in open(data_path)]

    def __getitem__(self, idx):
        item = self.data[idx]
        encoded = tokenizer(item['text'], max_length=128, padding='max_length')

        return {
            'input_ids': torch.tensor(encoded['input_ids']),
            'attention_mask': torch.tensor(encoded['attention_mask']),
            'soft_label': torch.tensor(item['soft_label'], dtype=torch.float32)
        }
```

### 6.2 è®­ç»ƒé…ç½®

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| Backbone | `roberta-base` | 768d hidden size |
| Bottleneck Dim | 64 | Information Bottleneck |
| Effective Batch Size | 256 | å¯¹æ¯”å­¦ä¹ çš„ç›®æ ‡ Batch å¤§å° |
| Physical Batch Size | 64 | å•æ¬¡å‰å‘ä¼ æ’­å¤§å° (è§†æ˜¾å­˜è°ƒæ•´) |
| Grad Accumulation | 4 | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° (Physical Ã— Accum = Effective) |
| Learning Rate (Backbone) | 2e-5 | é¢„è®­ç»ƒæ¨¡å‹å°LR |
| Learning Rate (Heads) | 1e-4 | æ–°å¤´å¯ä»¥ç”¨å¤§LR |
| Weight Decay | 0.01 | |
| $\lambda_{Cal}$ | 0.1 | Calibration lossæƒé‡ |
| $\lambda_{Aux}$ | 0.05 | Auxiliary lossæƒé‡ |

### 6.3 ç›‘æ§æŒ‡æ ‡

è®­ç»ƒæ—¶éœ€åŒæ—¶ç›‘æ§ï¼š

1. **Total Loss:** æ•´ä½“æ”¶æ•›æƒ…å†µ
2. **Average Kappa:** é¢„æµ‹çš„ä¸“æ³¨åº¦åˆ†å¸ƒ
   - é¢„æœŸï¼šå¼ºæ ·æœ¬ $\kappa \approx 40-50$ï¼Œå¼±æ ·æœ¬ $\kappa \approx 2-10$
3. **Auxiliary Accuracy:** 28åˆ†ç±»å‡†ç¡®ç‡ï¼ˆä»…ç”¨äºç›‘æ§ï¼Œéæœ€ç»ˆç›®æ ‡ï¼‰

---

## 7. Inference Strategy (æ¨ç†ç­–ç•¥)

### 7.1 å­˜å‚¨

å‘é‡æ•°æ®åº“ä¸­**ä»…å­˜å‚¨ $\boldsymbol{\mu}$ (64d)**ã€‚

### 7.2 æ£€ç´¢

```python
def search(query_text, index, top_k=10):
    # 1. Encode query
    outputs = model(query_text)
    mu_q = outputs['mu']        # (1, 64)
    kappa_q = outputs['kappa']  # (1, 1)

    # 2. Retrieve candidates (vector similarity)
    candidates = index.search(mu_q, top_k=100)

    # 3. Re-rank with mass-weighted score
    scores = kappa_q * torch.matmul(mu_q, candidates['mu'].T)

    return top_k_results
```

**æ ¸å¿ƒå…¬å¼ï¼š**

$$\text{Score}(q, d) = \kappa_q \cdot (\boldsymbol{\mu}_q^\top \boldsymbol{\mu}_d)$$

### 7.3 è¡Œä¸ºç‰¹æ€§

| Query ç±»å‹ | $\kappa_q$ å€¼ | æ£€ç´¢è¡Œä¸º |
|------------|---------------|----------|
| å¼ºæƒ…ç»ª (æš´æ€’) | $\approx 50$ | é«˜æ•æ„Ÿåº¦ï¼Œåªè¿”å›è¯­ä¹‰æœ€åŒ¹é…çš„ç»“æœ |
| å¼±æƒ…ç»ª (å¾®çƒ¦) | $\approx 5$ | ä½æ•æ„Ÿåº¦ï¼Œè¿”å›å¤šæ ·åŒ–çš„ç»“æœ |

---

## 8. Expected Contributions (å­¦æœ¯è´¡çŒ®)

1. **ç†è®ºè´¡çŒ®ï¼š** å°†æ–‡æœ¬åµŒå…¥ä»æ¬§æ°ç©ºé—´çš„ç‚¹å‡è®¾æ‰©å±•ä¸ºè¶…çƒé¢ä¸Šçš„ vMF åˆ†å¸ƒå‡è®¾ï¼Œå»ºç«‹äº†"ç‰©ç†è´¨é‡-ç»Ÿè®¡ä¸“æ³¨åº¦"çš„æ•°å­¦ç­‰ä»·æ€§ã€‚

2. **æ•°æ®åˆ›æ–°ï¼š** æå‡º **Soft Label Max-Norm** ä½œä¸ºå¼ºåº¦çš„å‡ ä½•å®šä¹‰ï¼Œä½¿æ¨¡å‹å­¦ä¹ å¯å¤ç°çš„åˆ†å¸ƒå‡ ä½•é‡è€Œéä¸»è§‚åˆ†æ•°ã€‚

3. **æ¶æ„åˆ›æ–°ï¼š** è®¾è®¡äº† **Bottlenecked Tri-Branch** ç»“æ„ï¼Œé€šè¿‡ä¿¡æ¯ç“¶é¢ˆæå–æ ¸å¿ƒè¯­ä¹‰ï¼ŒåŒæ—¶é€šè¿‡è¾…åŠ©æŸå¤±é˜²æ­¢è¯­ä¹‰åå¡Œã€‚

4. **æœºåˆ¶åˆ›æ–°ï¼š** æå‡ºäº† **Mass-Adaptive Temperature Scaling (MATS)**ï¼Œä» vMF æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ¨å¯¼å‡ºè‡ªé€‚åº”å¯¹æ¯”å­¦ä¹ æ¸©åº¦ï¼Œå¹¶è¯æ˜äº† $\alpha = 50.0$ çš„ç†è®ºæœ€ä¼˜æ€§ã€‚

---

## Appendix A: GoEmotions 28 Categories

The emotion categories used for soft label generation (28 total):

```python
EMOTIONS = [
    # Positive (12)
    "admiration", "amusement", "approval", "caring", "desire",
    "excitement", "gratitude", "joy", "love", "optimism",
    "pride", "relief",

    # Negative (11)
    "anger", "annoyance", "disappointment", "disapproval", "disgust",
    "embarrassment", "fear", "grief", "nervousness", "remorse",
    "sadness",

    # Ambiguous / Cognitive (4)
    "confusion", "curiosity", "realization", "surprise",

    # Neutral (1)
    "neutral"
]
```

**åˆ†å¸ƒè¯´æ˜ï¼š**
- **Positive (12):** ç§¯ææƒ…ç»ªï¼Œé€šå¸¸ä¼´éšé«˜å”¤é†’åº¦
- **Negative (11):** æ¶ˆææƒ…ç»ªï¼ŒåŒ…å«æ„¤æ€’ã€ææƒ§ã€æ‚²ä¼¤ç­‰
- **Ambiguous/Cognitive (4):** è®¤çŸ¥çŠ¶æ€ï¼Œå¯èƒ½ä¸ºæ­£å¯èƒ½ä¸ºè´Ÿ
- **Neutral (1):** ä¸­æ€§çŠ¶æ€ï¼Œä½œä¸ºåŸºå‡†ç±»åˆ«

---

*Document Version: V3 (NIPS Edition)*
*Last Updated: 2025*
